<!DOCTYPE html>




<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Trends in GPU price-performance" />
<meta name="author" content="Marius Hobbhahn" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Using a dataset of 470 models of graphics processing units released between 2006 and 2021, we find that the amount of floating-point operations/second per $ doubles every ~2.5 years." />
<meta property="og:description" content="Using a dataset of 470 models of graphics processing units released between 2006 and 2021, we find that the amount of floating-point operations/second per $ doubles every ~2.5 years." />
<link rel="canonical" href="https://epochai.org/blog/trends-in-gpu-price-performance" />
<meta property="og:url" content="https://epochai.org/blog/trends-in-gpu-price-performance" />
<meta property="og:site_name" content="Epoch" />
<meta property="og:image" content="https://epochai.org/assets/images/posts/2022/gpu-perf/banner.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-27T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://epochai.org/assets/images/posts/2022/gpu-perf/banner.png" />
<meta property="twitter:title" content="Trends in GPU price-performance" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Marius Hobbhahn"},"dateModified":"2022-06-27T00:00:00+00:00","datePublished":"2022-06-27T00:00:00+00:00","description":"Using a dataset of 470 models of graphics processing units released between 2006 and 2021, we find that the amount of floating-point operations/second per $ doubles every ~2.5 years.","headline":"Trends in GPU price-performance","image":"https://epochai.org/assets/images/posts/2022/gpu-perf/banner.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://epochai.org/blog/trends-in-gpu-price-performance"},"url":"https://epochai.org/blog/trends-in-gpu-price-performance"}</script>
<!-- End Jekyll SEO tag -->

  <title> Trends in GPU price-performance </title>

  
    






  

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css">
  <link rel="stylesheet" href="/assets/css/micromodal.css">
  <link rel="icon" type="image/svg+xml" href="/assets/images/favicon.svg" >
  <link rel="shortcut icon" type="image/png" href="/assets/images/favicon.png" ><!-- MathJax -->
  
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true,
      },

      options: {
        ignoreHtmlClass: 'tex2jax_ignore',
      },
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
  

  <link rel="stylesheet" href="/assets/css/main.css">

  <script src="/assets/js/micromodal.min.js"></script>

  <!-- Copy buttons -->
  <script src="/assets/js/clipboard.min.js"></script>
  <script>
  // TODO Temporary, compress
  function addCopyButton(node) {
    let copyButton;

    if (node.tagName == 'PRE') {
      copyButton = document.createElement('button');
      node.parentElement.appendChild(copyButton);
      node.parentElement.classList.add('copiable-wrapper');
      node.classList.add('copy-target');
      copyButton.classList.add('copy-button');
      copyButton.innerHTML = '<i class="bi-clipboard"></i>';
    } else {
      let wrapper = document.createElement('div');
      wrapper.classList.add('copiable-wrapper');
      node.parentNode.insertBefore(wrapper, node);
      wrapper.appendChild(node);

      copyButton = document.createElement('button');
      node.parentElement.appendChild(copyButton);
      node.parentElement.classList.add('copiable-wrapper');
      node.classList.add('copy-target');
      copyButton.classList.add('copy-button');
      copyButton.innerHTML = '<i class="bi-clipboard"></i>';
    }

    copyButton.querySelector('i').style.backgroundColor = node.style.backgroundColor;

    let clipboard = new ClipboardJS(copyButton, {
      target: function(trigger) {
        return trigger.parentElement.querySelector('.copy-target');
      },
    });

    let tooltip = tippy(copyButton, {
      content: 'Copied',
      trigger: 'manual',
      placement: 'left',
      appendTo: copyButton,
      arrow: false,
      offset: [1, -1],
    });

    clipboard.on('success', function(e) {
      e.clearSelection();

      let icon = e.trigger.querySelector('i');
      icon.classList.remove('bi-clipboard');
      icon.classList.add('bi-clipboard-check');

      tooltip.show();
      setTimeout(() => {
        tooltip.hide();
        icon.classList.remove('bi-clipboard-check');
        icon.classList.add('bi-clipboard');
      }, 1200);
    });

    clipboard.on('error', function(e) {
      e.clearSelection();
    });
  }

  window.addEventListener('DOMContentLoaded', () => {
    for (let element of document.querySelectorAll('pre, .boxed-text')) {
      addCopyButton(element);
    }
  });
</script>

</head>
<head>
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>

    
      

<script src="https://unpkg.com/@popperjs/core@2.11.2"></script>
<script src="https://unpkg.com/tippy.js@6.2.6"></script>



<style>
  :root {
    --resource-color: 78, 51, 80;
  }

  d-title {
    padding: 0;
  }

  d-title {
    margin-bottom: 15px;
  }

  d-article {
    border-top: 0;
    padding-top: 0;
  }

  .article-head {
    width: 100%;
    background-color: var(--header-color);
    margin-bottom: 4em;
  }

  .article-head-content {
    padding-left: var(--nav-bar-margin);
    padding-right: var(--nav-bar-margin);
    display: flex;
    margin: auto;
    padding-top: 70px;
    padding-bottom: 20px;
    box-sizing: border-box;
  }

  .page-content {
    padding-top: 0;
  }

  .summary-supertitle {
    margin-bottom: 0;
    margin-top: 10px;
    font-size: 0.7rem;
    text-transform: uppercase;
    word-spacing: 3px;
  }

  .summary-title {
    margin-bottom: 15px;
  }

  .summary-title h1 {
    margin-bottom: 0;
  }

  .cite-us {
    background-color: transparent;
    cursor: pointer;
    border: 0;
    font-size: 0.9rem;
    color: white !important;
  }

  .citation-tooltip {
    max-width: calc(100vw - 27px);
    box-shadow: 0 0 7px 7px black;
    border-radius: var(--default-radius);
  }

  .citation-tooltip > .tippy-box {
    padding-top: 0.5em;
  }

  .citation-tooltip pre * {
    color: var(--code-color);
  }

  @media (min-width: 800px) {
    .regular-banner .article-head-content {
      width: auto;
      height: calc(50vh);
      padding-top: 0;
      padding-bottom: 1em;
      padding-left: var(--nav-bar-margin);
      padding-right: var(--nav-bar-margin);
    }

    .regular-banner .banner-img-wrapper {
      width: 100%;
      height: 100%;
    }

    .regular-banner .banner-img-wrapper img {
      width: 100%;
      height: 100%;
      object-fit: cover;
      object-position: left;
    }

    .regular-banner .summary {
      flex: 0 0 50%;
      box-sizing: border-box;
    }

    .regular-banner .summary-main {
      width: 90%;
      margin: auto;
      margin-left: 0;
    }

    .regular-banner .banner {
      flex: 0 0 50%;
      margin: 0;
      padding: 0;
      object-fit: cover;
    }
}

  

  .summary {
    order: 1;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    padding-right: 1em;
  }

  .summary-title, .summary-title *,
  .summary-authors, .summary-authors *,
  .summary-abstract, .summary-abstract *,
  .summary-supertitle, .summary-supertitle *,
  .summary-footer, .summary-footer * {
    color: white;
  }

  .article-resources, .article-resources * {
    color: white !important;
  }

  .summary .copiable-wrapper i {
    color: black;
  }

  .summary-footer {
    margin-top: 2rem;
    font-size: 0.9rem;
  }

  .summary-authors {
    font-size: 0.9rem;
    margin-bottom: 1rem;
  }

  .banner {
    order: 2;
    flex: 0 0 41%;
    margin-left: auto;
    margin-right: auto;
    padding-left: 10px;
    //padding-right: 10px;
  }

  .banner-img-wrapper img {
    border-radius: var(--default-radius);
  }

  .article-resources {
    margin-top: 2rem;
    display: flex;
    flex-wrap: wrap;
  }

  .article-resource {
    border-radius: 4px;
    background-color: rgba(var(--resource-color), 0.8);
    padding: 6px 11px;
    font-size: 0.8rem;
    font-weight: bold;
    text-transform: uppercase;
    margin: 5px;
    margin-top: 0.5em;
    white-space: nowrap;
  }

  .article-resources i {
    margin-right: 0.5rem;
  }

  .article-resource:hover {
    background-color: rgba(var(--resource-color), 1.0);
  }

  @media (max-width: 800px) {
    .article-head-content {
      flex-wrap: wrap;
    }

    .banner, .regular-banner .banner {
      order: 1;
      flex-basis: 100%;
      margin-left: 0;
      margin-right: 0;
      padding-left: 0;
      text-align: left;
    }

    .summary {
      order: 2;
    }
  }
</style>

    
  </head>

  <body><header class="site-header" role="banner">
  <div class="header-wrapper"><a href="/">
      <img src="/assets/images/epoch-logo-white-text.svg" alt="Epoch logo" class="header-logo">
      
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
          
          <a class="page-link" href="/research">Research</a>
          <a class="page-link current-menu-item" href="/blog">Blog</a>
          <a class="page-link" href="/mlinputs/visualization">Visualization</a>
          <a class="page-link" href="/team">Team</a>
          <a class="page-link" href="/careers">Careers</a>
        </div>
      </nav></div>

  <script>
    document.addEventListener('click', (e) => {
      let navTrigger = document.querySelector('#nav-trigger');
      if (navTrigger && navTrigger.checked) {
        if (e.target != navTrigger.parentElement && !navTrigger.parentElement.contains(e.target)) {
          navTrigger.checked = false;
        }
      }
    });
  </script>

  <style>
    .image-tooltip {
      display: inline-block;
      width: 400px !important;
    }

    .image-tooltip img {
      width: 100%;
    }
  </style>

  <script>
    window.addEventListener('load', () => {
      for (let tooltipedElement of document.querySelectorAll('[data-tooltip-image]')) {
        let href = tooltipedElement.href;
        let imageUrl = tooltipedElement.dataset.tooltipImage;

        if (!imageUrl) continue;

        let content;
        if (href) {
          content = `<a class="image-tooltip" href="${href}"><img src='${imageUrl}'></img></a>`;
        } else {
          content = `<img class="image-tooltip" src='${imageUrl}'></img>`;
        }

        tippy(tooltipedElement, {
          allowHTML: true,
          placement: 'top',
          arrow: false,
          interactive: true,
          maxWidth: '500px',
          trigger: 'mouseenter',
          onShow: () => {
            tippy.hideAll();
          },
          content: content,
        });
      }
    });
  </script>

  <script>
    // Deobfuscate email addresses

    function addMailDeobfuscator(element) {
      element.addEventListener('mouseover', () => element.href = 'mailto:' + atob(element.dataset.contact));
      element.addEventListener('focus', () => element.href = 'mailto:' + atob(element.dataset.contact));
    }

    window.addEventListener('load', () => {
      for (let element of document.querySelectorAll("[data-contact]")) {
        addMailDeobfuscator(element);
      }
    });
  </script>
</header>
<main class="page-content" aria-label="Content">
      <div class="post distill">

        
          

<div class="article-head ">
  <div class="article-head-content">
    <div class="banner">
      <div class="banner-img-wrapper">
        <img src="/assets/images/posts/2022/gpu-perf/banner.png"/>
      </div>
    </div>
    <div class="summary">
      <div class="summary-main">
        
        <div class="summary-title">
          <h1>Trends in GPU price-performance</h1>
          <a class="cite-us"><i class="bi-journal-text"></i> Cite this post</a>
        </div>
        
        <div class="summary-authors">
          
          



      
      <span class="author tooltiped" data-member-id="marius-hobbhahn">Marius Hobbhahn</span> and 
      
      <span class="author tooltiped" data-member-id="tamay-besiroglu">Tamay Besiroglu</span>

<script src="/assets/js/umbrella.min.js"></script>
<style>
  .author.tooltiped {
    cursor: pointer;
  }

  .summary-authors .tippy-content {
    width: 180px;
    padding: 5px;
  }

  .miniprofile .mug {
    width: 180px;
    height: 180px;
    background-size: cover;
    background-position: center;
  }

  .miniprofile a {
    color: black;
    text-decoration: none;
  }

  .miniprofile .member-resource {
    margin-right: 5px;
  }

  .miniprofile .member-info {
    padding: 4px;
    width: 180px;
  }

  .miniprofile .member-name, .miniprofile .member-role {
    margin-bottom: 2px;
  }
</style>

<script>

  

  

  let teamMembers = {
    
      'jaime-sevilla': {
        id: 'jaime-sevilla',
        name: 'Jaime Sevilla',
        description: 'Jaime is a researcher focused on statistics and technological forecasting. Besides his role at Epoch, he is a research affiliate of the <a href="https://www.cser.ac.uk/">Centre for the Study of Existential Risk</a> at Cambridge University and the cofounder of <a href="https://riesgoscatastroficosglobales.com/">Riesgos Catastróficos Globales</a>.',
        role: 'Director',
        imageUrl: '/assets/images/team/jaime-sevilla.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'amFpbWVAZXBvY2hhaS5vcmc='},
        
          
          
          
          
          
          {name: 'twitter', icon: 'twitter', url: 'https://twitter.com/Jsevillamol'},
        
        ],
      },
    
      'tamay-besiroglu': {
        id: 'tamay-besiroglu',
        name: 'Tamay Besiroglu',
        description: 'Tamay is a researcher focusing on the Economics of Computing and big-picture trends in Machine Learning. In addition to his role at Epoch, Tamay is a researcher at the Future Tech Lab at MIT, and AI Forecasting Lead at Metaculus. Previously, he led strategy for Metaculus, consulted for the UK Government, and worked at the Future of Humanity Institute.',
        role: 'Associate director',
        imageUrl: '/assets/images/team/tamay-besiroglu.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'dGFtYXlAZXBvY2hhaS5vcmc='},
        
        ],
      },
    
      'pablo-villalobos': {
        id: 'pablo-villalobos',
        name: 'Pablo Villalobos',
        description: 'Pablo has a background in Mathematics and Computer Science. After spending some time as a software engineer, he decided to pivot towards AI. His interests include the economic consequences of advanced AI systems and the role of algorithmic improvements in AI progress.',
        role: 'Staff Researcher',
        imageUrl: '/assets/images/team/pablo-villalobos.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'cGFibG9AZXBvY2hhaS5vcmc='},
        
        ],
      },
    
      'anson-ho': {
        id: 'anson-ho',
        name: 'Anson Ho',
        description: 'Anson is a researcher and writer for Epoch. His research interests are in interpretability, theoretical AI alignment, and ensuring safe AI development through governance and strategy. Prior to this, he completed his BSc in physics at the University of St Andrews.',
        role: 'Staff Researcher',
        imageUrl: '/assets/images/team/anson-ho.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'YW5zb25AZXBvY2hhaS5vcmc='},
        
          
          
          
          
          
          {name: 'github', icon: 'github', url: 'https://github.com/ansonwhho'},
        
          
          
          
          
          
          {name: 'website', icon: 'globe', url: 'https://ansonwhho.github.io/'},
        
        ],
      },
    
      'lennart-heim': {
        id: 'lennart-heim',
        name: 'Lennart Heim',
        description: 'Lennart is a researcher on AI and compute. His research interests include the role of compute in the AI production function, the compute landscape/supply chain, security of AI systems, and forecasting emerging technologies. He is a research affiliate with the Centre for the Governance of AI in Oxford and has a background in Computer Engineering.',
        role: 'Research Fellow and Strategy Specialist',
        imageUrl: '/assets/images/team/lennart-heim.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'bGVubmFydEBlcG9jaGFpLm9yZw=='},
        
          
          
          
          
          
          {name: 'twitter', icon: 'twitter', url: 'https://twitter.com/ohlennart'},
        
          
          
          
          
          
          {name: 'website', icon: 'globe', url: 'https://heim.xyz'},
        
        ],
      },
    
      'marius-hobbhahn': {
        id: 'marius-hobbhahn',
        name: 'Marius Hobbhahn',
        description: 'Marius builds models for AI timelines and takeoff using historical trends and his best understanding of the future.',
        role: 'Research Fellow',
        imageUrl: '/assets/images/team/marius-hobbhahn.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'bWFyaXVzQGVwb2NoYWkub3Jn'},
        
        ],
      },
    
      'eduardo-infante-roldan': {
        id: 'eduardo-infante-roldan',
        name: 'Eduardo Infante-Roldán',
        description: 'Eduardo does some programming.',
        role: 'Software Engineer',
        imageUrl: '/assets/images/team/eduardo-infante-roldan.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'ZWR1QGVwb2NoYWkub3Jn'},
        
        ],
      },
    
      'ege-erdil': {
        id: 'ege-erdil',
        name: 'Ege Erdil',
        description: 'Ege Erdil is an undergraduate student at Middle East Technical University. He has interests in mathematics, statistics, economics and forecasting.',
        role: 'Intern Researcher',
        imageUrl: '/assets/images/team/ege-erdil.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'ZWdlQGVwb2NoYWkub3Jn'},
        
        ],
      },
    
  };

  

  

  let advisors = {
    
      'tom-davidson': {
        id: 'tom-davidson',
        name: 'Tom Davidson',
        description: 'Tom is a senior research analyst at Open Philanthropy. He’s currently working on assessing arguments that transformative AI might be developed relatively soon. Prior to joining Open Philanthropy, Tom worked as a Data Scientist for education technology startup BridgeU and taught science at a UK comprehensive school. He has a Masters in Physics and Philosophy from the University of Oxford.',
        role: 'Research Advisor',
        imageUrl: '/assets/images/advisors/tom-davidson.jpg',
        resources: [
        
        ],
      },
    
      'neil-thompson': {
        id: 'neil-thompson',
        name: 'Neil Thompson',
        description: 'Neil is an Innovation Scholar at MIT’s Computer Science and Artificial Intelligence Lab and the Initiative on the Digital Economy where he leads the FutureTech Project. He is also an Associate Member of the Broad Institute.',
        role: 'Research Advisor',
        imageUrl: '/assets/images/advisors/neil-thompson.jpg',
        resources: [
        
        ],
      },
    
  };

  

  let members = {...teamMembers, ...advisors};
</script>


<script>
  for (let author of document.querySelectorAll('.author.tooltiped')) {
    let member = teamMembers[author.dataset.memberId];
    if (!member) continue;

    let resources = [];
    for (let resource of member.resources) {
      if (resource.name == "mail") {
        resources.push(`<a class="member-resource" href="#" data-contact="${resource.url}"><i class="bi bi-${resource.icon}"></i></a>`);
      } else {
        resources.push(`<a class="member-resource" href="${resource.url}"><i class="bi bi-${resource.icon}"></i></a>`);
      }
    }

    tippy(author, {
      allowHTML: true,
      placement: 'top',
      arrow: false,
      interactive: true,
      maxWidth: '200px',
      trigger: 'mouseenter click',
      onShow: (instance) => {
        tippy.hideAll();
        for (let element of instance.popper.querySelectorAll('[data-contact]')) {
          addMailDeobfuscator(element);
        }
      },
      content:
        `
        <div class="miniprofile">
          <div class="mug" style="border-radius: 5px; background-image: url('${member.imageUrl}')"></div>
          <div class="member-info">
            <h3 class="member-name">${member.name}</h3>
            <h4 class="member-role">${member.role}</h4>
            <div class="member-resources">
              ${resources.join('\n')}
            </div>
          </div>
        </div>
        `,
    });
  }
</script>

        </div>
        
        
        <div class="summary-abstract">
          Using a dataset of 470 models of graphics processing units released between 2006 and 2021, we find that the amount of floating-point operations/second per $ doubles every ~2.5 years.
        </div>
        
        
      </div>
      
      <div class="summary-footer">
        Jun. 27, 2022
      </div>
      
    </div>
  </div>
</div>







<script>
  let content = `
    
<p style="margin-bottom: 0">Cite this work as</p>

<div class="language-plaintext highlighter-rouge wrappable-pre">
<div class="highlight">
  <pre class="highlight"><code>Marius Hobbhahn and Tamay Besiroglu (2022), "Trends in GPU price-performance". <em>Published online at epochai.org.</em> Retrieved from: 'https://epochai.org/blog/trends-in-gpu-price-performance' [online resource]</code></pre>
</div>
</div>

<p style="margin-bottom: 0">BibTeX citation:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{epoch2022trendsingpupriceperformance,
  title = "Trends in GPU price-performance",
  author = {Marius Hobbhahn and Tamay Besiroglu},
  year = 2022,
  howpublished = "\\url{https://epochai.org/blog/trends-in-gpu-price-performance}",
  note = "Accessed: 2022-07-19"
}
</code></pre></div></div>

  `;

  tippy('.cite-us', {
    allowHTML: true,
    placement: 'bottom-start',
    arrow: false,
    interactive: true,
    maxWidth: '900px',
    trigger: 'click',
    content: content,
    onShow: (instance) => {
      instance.popper.classList.add('citation-tooltip');
      tippy.hideAll();
      instance.popper.querySelectorAll('pre').forEach(pre => addCopyButton(pre));
    },
  });
</script>


        

        <d-article>
          <d-contents>
            <nav class="l-text figcaption">
            <h3>Contents</h3><div><ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#executive-summary">Executive Summary</a></li>
<li class="toc-entry toc-h1"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h1"><a href="#dataset">Dataset</a></li>
<li class="toc-entry toc-h1"><a href="#empirical-analysis">Empirical analysis</a>
<ul>
<li class="toc-entry toc-h2"><a href="#empirical-trend-vs-other-predictions">Empirical trend vs. other predictions</a></li>
<li class="toc-entry toc-h2"><a href="#trends-across-precision-for-floating-formats">Trends across precision for floating formats</a></li>
<li class="toc-entry toc-h2"><a href="#trends-of-gpus-used-in-ml">Trends of GPUs used in ML</a></li>
<li class="toc-entry toc-h2"><a href="#trend-of-top-performing-gpus">Trend of top-performing GPUs</a></li>
<li class="toc-entry toc-h2"><a href="#all-trends-table--figure">All trends (table &amp; figure)</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#conclusion">Conclusion</a></li>
<li class="toc-entry toc-h1"><a href="#appendix-a---dropping-data-before-2006">Appendix A - dropping data before 2006</a></li>
<li class="toc-entry toc-h1"><a href="#appendix-b---robustness-check-for-flops">Appendix B - Robustness check for FLOP/s</a>
<ul>
<li class="toc-entry toc-h3"><a href="#more-flops-plots">More FLOP/s plots</a></li>
</ul>
</li>
</ul></div></nav>
          </d-contents>

          <style>
  .figure-flexbox {
    display: flex;
    flex-wrap: wrap;
  }

  .figure-flexbox figure {
    max-width: calc(min(100%, 345px));
  }

  @media (max-width: 1180px) {
    max-width: calc(min(100%, 490px));
  }
</style>

<p><em>We would like to thank Alyssa Vance, Ashwin Acharya, Jessica Taylor and the Epoch team for helpful feedback and comments.</em></p>

<p><br /></p>

<h1 id="executive-summary">Executive Summary</h1>

<p>Using a dataset of 470 models of graphics processing units (GPUs) released between 2006 and 2021, we find that the amount of floating-point operations/second per $ (hereafter FLOP/s per $) doubles every ~2.5 years. For top GPUs at any point in time, we find a slower rate of improvement (FLOP/s per $ doubles every 2.95 years), while for models of GPU typically used in ML research, we find a faster rate of improvement (FLOP/s per $ doubles every 2.07 years). GPU price-performance improvements have generally been slightly slower than the 2-year doubling time associated with Moore’s law, much slower than what is implied by Huang’s law, yet considerably faster than was generally found in prior work on trends in GPU price-performance. We aim to provide a more precise characterization of GPU price-performance trends based on more or higher-quality data, that is more robust to justifiable changes in the analysis than previous investigations.<sup id="fnref:a" role="doc-noteref"><a href="#fn:a" class="footnote" rel="footnote">1</a></sup></p>

<figure>
  <img src="/assets/images/posts/2022/gpu-perf/image3.png" />
  <figcaption class="caption">
    <p>Figure 1. Plots of FLOP/s and FLOP/s per dollar for our dataset and relevant trends from the existing literature</p>
  </figcaption>
</figure>

<table>
  <thead>
    <tr>
      <th><p><span>Trend</span></p></th>
      <th><p><span>2x time</span></p></th>
      <th><p><span>10x time</span></p></th>
      <th><p><span>Metric</span></p></th>
    </tr>
  </thead>
  <tbody>
    <tr style="background-color: #d9d2e9">
      <td><p><span>Our dataset<br />(n=470)</span></p></td>
      <td><p><span>2.46 years <br />[2.24, 2.72]</span></p></td>
      <td><p><span>8.17 years <br />[7.45, 9.04]</span></p></td>
      <td><p><span>FLOP/s per dollar</span></p></td>
    </tr>
    <tr style="background-color: #f6b26b">
      <td><p><span>ML GPUs<br />(n=26)</span></p></td>
      <td><p><span>2.07 years<br />[1.54, 3.13]</span></p></td>
      <td><p><span>6.86 years<br />[5.12, 10.39]</span></p></td>
      <td><p><span>FLOP/s per dollar</span></p></td>
    </tr>
    <tr style="background-color: #b7b7b7">
      <td> <p> <span> Top GPUs<br /> (n=57) </span> </p> </td>
      <td> <p> <span> 2.95 years<br /> [2.54, 3.52] </span> </p> </td>
      <td> <p> <span> 9.81 years<br /> [8.45, 11.71] </span> </p> </td>
      <td> <p><span>FLOP/s per dollar</span></p> </td>
    </tr>
    <tr style="background-color: #a2c4c9">
      <td> <p><span>Our data FP16 (n=91)</span></p> </td>
      <td> <p> <span> 2.30 years<br /> [1.69, 3.62] </span> </p> </td>
      <td> <p> <span> 7.64 years<br /> [5.60, 12.03] </span> </p> </td>
      <td> <p><span>FLOP/s per dollar</span></p> </td>
    </tr>
    <tr style="background-color: #d9ead3">
      <td> <p><span>Moore’s law</span></p> </td>
      <td> <p><span>2 years</span></p> </td>
      <td> <p><span>6.64 years</span></p> </td>
      <td> <p><span>FLOP/s</span></p> </td>
    </tr>
    <tr style="background-color: #f4cccc">
      <td> <p><span>Huang’s law</span></p> </td>
      <td> <p><span>1.08 years</span></p> </td>
      <td> <p><span>3.58 years</span></p> </td>
      <td> <p><span>FLOP/s</span></p> </td>
    </tr>
    <tr style="background-color: #cfe2f3">
      <td>
        <p>
          <span>CPU historical (<a href="https://aiimpacts.org/trends-in-the-cost-of-computing/">AI Impacts, 2019</a>)</span>
        </p>
      </td>
      <td>
        <p><span>2.32 years</span></p>
      </td>
      <td>
        <p><span>7.7 years</span></p>
      </td>
      <td>
        <p><span>FLOP/s per dollar</span></p>
      </td>
    </tr>
    <tr style="background-color: #cfe2f3">
      <td>
        <p>
          <span>
            <a href="https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/">
                Bergal, 2019
            </a>
          </span>
        </p>
      </td>
      <td>
        <p><span>4.4 years</span></p>
      </td>
      <td>
        <p><span>14.7 years</span></p>
      </td>
      <td>
        <p><span>FLOP/s per dollar</span></p>
      </td>
    </tr>
  </tbody>
  <caption>
    Table 1. Summary of our findings on GPU price-performance trends and relevant trends in the existing literature with the 95% confidence intervals in square brackets.
  </caption>
</table>

<h1 id="introduction">Introduction</h1>

<p>GPUs are the dominant computing platform for accelerating machine learning (ML) workloads, and most (if not all) of the biggest models over the last five years have been trained on GPUs or other special-purpose hardware like tensor processing units (TPUs). Price-performance improvements in underlying hardware has resulted in a rapid growth of the size of ML training runs (<a href="https://arxiv.org/abs/2202.05924">Sevilla et al., 2022</a>), and has thereby centrally contributed to the recent progress in AI.</p>

<p>The rate at which GPUs have been improving has been analyzed previously. For example, <a href="https://www.semanticscholar.org/paper/Multi-chip-technologies-to-unleash-computing-gains-Su-Naffziger/7ff96079f20fc5dbb399be8c6189464ef990692a">Su et al., 2017</a> finds a 2.4-year doubling rate for GPU FLOP/s from 2006 to 2017. <a href="https://arxiv.org/abs/1911.11313">Sun et al., 2019</a> analyses over 4,000 GPU models and finds that FLOPs per watt doubles around every three to four years. By contrast, some have speculated that GPU performance improvements are more rapid than the exponential improvements associated with other microprocessors like CPUs (which typically see a 2 to 3-year doubling time, see <a href="https://aiimpacts.org/trends-in-the-cost-of-computing/">AI Impacts, 2019</a>). Notable amongst these is the so-called Huang’s Law proposed by NVIDIA CEO, Jensen Huang, according to whom GPUs see a “25x improvement every 5 years” (<a href="https://www.wsj.com/articles/huangs-law-is-the-new-moores-law-and-explains-why-nvidia-wants-arm-11600488001">Mims, 2020</a>), which would be equivalent to a ~1.1-year doubling time in performance.</p>

<p>There is previous work that specifically analyzes price-performance across CPUs and GPUs (summarized in Table 1). Prior estimates of the rate of improvement vary widely (e.g. the time it takes for price-performance to increase by 10-fold ranges from ~6 to ~15 years, depending on the computing precision—see Table 2.). Due to the high variance of previous approaches and their usage of smaller datasets, we are not confident in existing estimates.<sup id="fnref:b" role="doc-noteref"><a href="#fn:b" class="footnote" rel="footnote">2</a></sup></p>

<style>
  @media (min-width: 1180px) {
    #table-2 tr th:nth-child(2) {
      width: 8em;
    }
  }
</style>

<table id="table-2">
  <thead>
    <tr>
      <th><strong>Reference</strong></th>
      <th><strong>Processor type</strong></th>
      <th><strong>Metric</strong></th>
      <th><strong>2x time</strong></th>
      <th><strong>10x time</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr style="background-color: #f4cccc">
      <td><a href="https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/">Bergal, 2019</a></td>
      <td>GPU</td>
      <td>FLOP/s per $ in FP32, FP16, and FP16 fused multiply-add</td>
      <td>4.4 years (FP32)<br /> 3.0 years (FP16)<br /> 1.8 years (FP16 fused)</td>
      <td>14.7 years (FP32)<br /> 10.0 years (FP16)<br /> 6.1 years (FP16 fused)</td>
    </tr>
    <tr style="background-color: #fff2cc">
      <td><a href="http://mediangroup.org/docs/Feasibility%20of%20Training%20an%20AGI%20using%20Deep%20Reinforcement%20Learning,%20A%20Very%20Rough%20Estimate.pdf">Median Group, 2018</a></td>
      <td>GPU</td>
      <td>FLOP/s per $ in FP32</td>
      <td>1.5 years</td>
      <td>5.0 years</td>
    </tr>
    <tr style="background-color: #d9ead3">
      <td><a href="https://intelligence.org/2014/05/12/exponential-and-non-exponential/#footnote_7_11027">Muehlhauser and Rieber, 2014</a></td>
      <td>Various</td>
      <td>MIPS/$</td>
      <td>1.6 years</td>
      <td>5.2 years</td>
    </tr>
    <tr style="background-color: #c9daf8">
      <td><a href="http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf">Sandberg and Bostrom, 2008</a></td>
      <td>CPU-based</td>
      <td>MIPS/$ and FLOP/s per $</td>
      <td>1.7 years (MIPS)<br /> 2.3 (FLOP/s)</td>
      <td>5.6 years (MIPS)<br /> 7.7 years (FLOP/s)</td>
    </tr>
    <tr style="background-color: #d9d2e9">
      <td><a href="https://web.archive.org/web/20160222082744/http://www.econ.yale.edu/~nordhaus/homepage/prog_083001a.pdf">Nordhaus, 2001</a></td>
      <td>CPU-based</td>
      <td>MIPS/$</td>
      <td>1.6 years</td>
      <td>5.3 years</td>
    </tr>
  </tbody>
  <caption class="caption">
    Table 2. Price-performance improvements found in prior work. See also <a href="https://aiimpacts.org/trends-in-the-cost-of-computing/#Evidence">AI Impacts 2015</a> for a more detailed overview of prior estimates.
  </caption>
</table>

<p>We aim to extend the existing work with three main contributions:</p>

<ol>
  <li>Using a larger dataset of GPU models than has been analyzed in previous investigations that includes more recent GPU models, we produce more precise estimates of the rate of price-performance improvements for GPUs than currently exists<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">3</a></sup></li>
  <li>We analyze multiple key subtrends for GPU price-performance improvements, such as the trends in price-performance for top-performing GPU and for GPUs commonly used for Machine Learning</li>
  <li>We put the trends into perspective by comparing them against prior estimates, Moore’s law, Huang’s law, prior analyses, and public predictions on GPU performance</li>
</ol>

<h1 id="dataset">Dataset</h1>

<p>We combine two existing datasets on GPU price-performance. One dataset is from the Median Group, which contains data on 223 Nvidia and AMD GPUs (<a href="http://mediangroup.org/docs/Feasibility%20of%20Training%20an%20AGI%20using%20Deep%20Reinforcement%20Learning,%20A%20Very%20Rough%20Estimate.pdf">Median Group, 2018</a>). The second dataset is from <a href="https://arxiv.org/pdf/1911.11313.pdf">Sun et al., 2019,</a> which contains price-performance data on 413 GPUs released by Nvidia, Intel and AMD.</p>

<figure>
  <img src="/assets/images/posts/2022/gpu-perf/image1.png" />
  <figcaption class="caption">
    <p>Figure 2. Plots of FLOP/s and FLOP/s per dollar for Median Group’s and <a href="https://arxiv.org/pdf/1911.11313.pdf">Sun et al., 2019</a>’s datasets.</p>
  </figcaption>
</figure>

<p>We merged both datasets and removed duplicate observations, i.e. GPU models that were contained in both datasets. Furthermore, we removed different versions of the same product unless they had different specifications.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">4</a></sup></p>

<p>We also decided to drop observations prior to 2006 for two main reasons: 1) it is unclear whether the we can meaningfully compare their levels of performance as these models predate innovations that enable general-purpose computing on GPUs, and 2) we were not able to validate the accuracy of the data by looking up the relevant performance details in models’ data sheets. For a more detailed discussion see <a href="#appendix-a---dropping-data-before-2006">Appendix A</a>.</p>

<p>Finally, we noticed that there is a subset of 20 GPUs for which the 16-bit performance is ~60-fold worse than its performance in 32-bit format, while for all other GPUs the 16-bit performance is at least as good as its 34-bit performance. We dropped these 16-bit performance numbers, which we think might have been erroneous.</p>

<p>The final dataset thus contains 470 GPUs from AMD, Intel, and Nvidia released between 2006 and 2021. We will refer to this merged dataset as “our dataset” for the rest of the report. Throughout, FLOP/s are those in 32-bit (full) precision.</p>

<figure>
  <img src="/assets/images/posts/2022/gpu-perf/image11.png" />
  <figcaption class="caption">
    <p>Figure 3. Plots of FLOP/s and FLOP/s per dollar for the dataset used in our analysis</p>
  </figcaption>
</figure>

<h1 id="empirical-analysis">Empirical analysis</h1>

<p>In what follows, we analyze trends in price-performance, measured in FLOP/s per dollar as well as raw performance in FLOP/s for GPUs in our dataset. Our analysis considers key subsets, such as GPUs commonly used in Machine Learning research, as well as top-performing GPUs.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">5</a></sup> </p>

<h2 id="empirical-trend-vs-other-predictions">Empirical trend vs. other predictions</h2>

<p>To put our findings in context, we compare them with other proposed GPU (price) performance trends found elsewhere. These are</p>

<ul>
  <li>Moore’s law, which states that a transistor density doubled every two years. For the purpose of comparison, we take that to mean that the amount of FLOP/s also doubles every two years</li>
  <li><a href="https://en.wikipedia.org/wiki/Huang%27s_law">Huang’s law</a>, which describes the rate of performance improvements for GPUs. While there are multiple interpretations of Huang’s law, we chose the one that reflects Huang’s original wording, namely “25x improvement every 5 years”</li>
  <li>Historical trends in CPU price-performance, which has been found to increase by a factor of 10 every 7.7 years since 1940 (<a href="https://aiimpacts.org/trends-in-the-cost-of-computing/">AI Impacts, 2019</a>)</li>
  <li>The prediction made in <a href="https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">Cotra 2020</a> of a 2.5-year doubling time in price-performance of compute relevant to Machine Learning training runs<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">6</a></sup></li>
  <li>Prior estimates of the rate of GPU price-performance found by <a href="https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/">Bergal, 2019</a></li>
</ul>

<p>We recognize that some of these trends are not quite comparable to FLOP/s per $ (Moore’s law relates to the density of circuits, Huang’s law relates to theoretical performance improvements, while <a href="https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">Cotra 2020</a>’s predictions relates to FLOP per dollar<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">7</a></sup>). The purpose of these comparisons is just to provide a rough sense of how our estimated trends relate to relevant empirical trends and predictions.</p>

<p>Unless specified otherwise, we will present the results for FLOP/s per dollar. This is because we a) think that FLOP/s per dollar is the more relevant trend as argued previously and b) because there is not that much of a difference between FLOP/s and FLOP/s per dollar trends. For a detailed comparison see <a href="#appendix-b---robustness-check-for-flops">Appendix B</a>.</p>

<figure>
  <img style="max-width: calc(min(100%, 600px))" src="/assets/images/posts/2022/gpu-perf/image5.png" />
  <figcaption class="caption">
    <p>Figure 4. FLOP/s per dollar for our dataset and relevant trends found elsewhere</p>
  </figcaption>
</figure>

<p>We find that a linear regression through all of our data shows a doubling time of 2.46 years (95% CI: 2.24 to 2.72 years). This is very well in line with the slope of 2.5 used in <a href="https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">Cotra 2020</a>. We can also see that Huang’s law is not a good fit for the entire trend and is strongly overstated.</p>

<h2 id="trends-across-precision-for-floating-formats">Trends across precision for floating formats</h2>

<p>Half-precision computing (FP16) and mixed-precision computing (usually FP16 and FP32) are now commonly used for deep learning. In our dataset, we had 91 GPUs for which we had both price and FP16 performance numbers.<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">8</a></sup> </p>

<figure>
  <img style="max-width: calc(min(100%, 600px))" src="/assets/images/posts/2022/gpu-perf/image9.png" />
  <figcaption class="caption">
    <p>Figure 5. FLOP/s per dollar for FP32 and FP16 performance</p>
  </figcaption>
</figure>

<p>We find that the price-performance doubling time in FP16 was 2.32 years (95% CI: 1.69 years, 3.62 years). This was not significantly different from the slope for the doubling time for price-performance in FP32, suggesting that price-performance improvements in FP16 and FP32 are likely to be similar. This stands in contrast to the findings of <a href="https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/">Bergal, 2019</a>, which finds a 1.8-year doubling time for FP16 FMA.<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">9</a></sup> In what follows, we decide to focus on price-performance in FP32 as we do not find a statistically significant difference between the two trends, and we therefore choose to analyze the models for which we have the most data on.</p>

<h2 id="trends-of-gpus-used-in-ml">Trends of GPUs used in ML</h2>

<p>The vast majority of all ML training is done on a very small number of different models of GPUs. From a <a href="https://arxiv.org/abs/2202.05924">previous publication</a> where we looked at 75 papers that present milestone ML models, we collected a total of 42 distinct models of GPUs commonly used to train ML systems. In total, we found 26 of these 42 GPUs in our dataset on GPUs.</p>

<p>We find that the price-performance of GPUs used in ML improves faster than the typical GPU. We find that FLOP/s per dollar for ML GPUs double every 2.07 years (95% CI: 1.54 to 3.13 years) compared to 2.46 years for all GPUs. This is not significantly different from the slope for the doubling time for price-performance for all GPUs.</p>

<figure>
  <img style="max-width: calc(min(100%, 620px))" src="/assets/images/posts/2022/gpu-perf/image12.png" />
  <figcaption class="caption">
    <p>Figure 6. FLOP/s per dollar for our dataset and separately for GPU models commonly used in ML research compared to relevant trends found elsewhere</p>
  </figcaption>
</figure>

<p>Furthermore, the latest ML GPUs tend to be among the GPUs with high price-performance, whereas the older ones are more middle of the pack.</p>

<p>Moreover, when looking through the FLOP/s lens, it becomes even more clear that the latest ML experiments use the most powerful GPUs. We think that shows the increased importance of GPUs for modern ML. Once again, the ML GPUs show a steeper slope than the general trend (doubling time of 2.00 years compared to 2.31 years for all GPUs).</p>

<p>Our higher point estimate for the rate of performance improvements amongst GPUs used for ML research could be explained by relevant labs spending more resources on procuring top GPUs over time. If this were the case, this would reflect merely a change in investment decisions by relevant research labs and not a faster-than-usual rate of improvement of the underlying hardware amongst the relevant GPUs suitable for ML workloads. Given this, and because our estimates for the entire GPUs is not statistically significantly different, we expect that the ~2.5 year doubling time to be a more reliable estimate of the underlying rate of hardware price-performance improvements.</p>

<h2 id="trend-of-top-performing-gpus">Trend of top-performing GPUs</h2>

<p>As we saw in the previous section, the latest ML models tend to be trained on state-of-the-art GPUs. Therefore, looking at the trend of top-performing GPUs might be a good indicator for ML capabilities in the future. Note, that this does not imply that we think that GPU performance will grow linearly. We will publish more detailed thoughts on predictions from this data in a second piece.</p>

<p>Here, we select the subset of GPUs that had the highest FLOP/s per dollar values during each month. For this subset of models, we find a doubling time of 2.95 years (95% CI: 2.54 to 3.52 years), which is statistically significantly longer than the typical doubling time.</p>

<figure>
  <img style="max-width: calc(min(100%, 620px))" src="/assets/images/posts/2022/gpu-perf/image10.png" />
  <figcaption class="caption">
    <p>Figure 7. FLOP/s per dollar for our dataset and separately for top-performing GPUs compared to relevant trends found elsewhere</p>
  </figcaption>
</figure>

<h2 id="all-trends-table--figure">All trends (table &amp; figure)</h2>

<p>To compare all the trends we highlighted above and the ones you can find in the appendix, we collected all trends, and for each, report the associated time it takes to increase 2x and 10x.</p>

<style>
  @media (min-width: 1180px) {
    #table-3 tr th:nth-child(2) {
      width: 6em;
    }
  }
</style>

<table id="table-3">
  <thead>
    <tr>
      <th>Trend</th>
      <th>Original presentation</th>
      <th>2x time</th>
      <th>10x time</th>
      <th>Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr style="background-color: #d9ead3">
      <td>Moore’s law</td>
      <td>2x every 2 years</td>
      <td>2 years</td>
      <td>6.64 years</td>
      <td>FLOP/s</td>
    </tr>
    <tr style="background-color: #f4cccc">
      <td>Huang’s law</td>
      <td>25x every 5 years</td>
      <td>1.08 years</td>
      <td>3.58 years</td>
      <td>FLOP/s</td>
    </tr>
    <tr style="background-color: #d0e0e3">
      <td>Biological anchors report (<a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines&amp;sa=D&amp;source=editors&amp;ust=1656344719876336&amp;usg=AOvVaw2i9SKlYv4rV31-zjyBCI1V">Cotra, 2020</a>)</td>
      <td>2x every 2.5 years</td>
      <td>2.5 years</td>
      <td>8.30 years</td>
      <td>FLOP/s per dollar</td>
    </tr>
    <tr style="background-color: #cfe2f3">
      <td>CPU historical (<a href="https://aiimpacts.org/trends-in-the-cost-of-computing/&amp;sa=D&amp;source=editors&amp;ust=1656344719878007&amp;usg=AOvVaw1tSSji93R6BI_tQm87PjPW">AI Impacts, 2019</a>)</td>
      <td>10x every 7.7 years</td>
      <td>2.32 years</td>
      <td>7.7 years</td>
      <td>FLOP/s per dollar</td>
    </tr>
    <tr style="background-color: #fff2cc">
      <td><a href="http://mediangroup.org/docs/Feasibility%20of%20Training%20an%20AGI%20using%20Deep%20Reinforcement%20Learning,%20A%20Very%20Rough%20Estimate.pdf">Median Group, 2018</a></td>
      <td>2x every 1.5 years</td>
      <td>1.5 years</td>
      <td>1.5 years</td>
      <td>FLOP/s per dollar</td>
    </tr>
    <tr style="background-color: #d9d2e9">
      <td>Our data (n=470)</td>
      <td>-</td>
      <td>2.46 years [2.24, 2.72]</td>
      <td>8.17 years [7.45, 9.04]</td>
      <td>FLOP/s per dollar</td>
    </tr>
    <tr style="background-color: #d9d2e9">
      <td>Our data (n=470)</td>
      <td>-</td>
      <td>2.31 years[2.14, 2.51]</td>
      <td>7.68 years[7.12, &nbsp;8.33]</td>
      <td>FLOP/s</td>
    </tr>
    <tr style="background-color: #a2c4c9">
      <td>Our data FP16 (n=91)</td>
      <td>-</td>
      <td>2.30 years[1.69, 3.62]</td>
      <td>7.64 years[5.60, 12.03]</td>
      <td>FLOP/s per dollar</td>
    </tr>
    <tr style="background-color: #a2c4c9">
      <td>Our data FP16 (n=91)</td>
      <td>-</td>
      <td>2.91 years[1.94, 5.83]</td>
      <td>9.68 years[6.45, 19.35]</td>
      <td>FLOP/s</td>
    </tr>
    <tr style="background-color: #f6b26b">
      <td>ML GPUs (n=26)</td>
      <td>-</td>
      <td>2.07 years[1.54, 3.13]</td>
      <td>6.86 years[5.12, 10.39]</td>
      <td>FLOP/s per dollar</td>
    </tr>
    <tr style="background-color: #f6b26b">
      <td>ML GPUs (n=26)</td>
      <td>-</td>
      <td>2.00 years[1.69, 2.43]</td>
      <td>6.63 years[5.63, &nbsp;8.07]</td>
      <td>FLOP/s</td>
    </tr>
    <tr style="background-color: #b7b7b7">
      <td>Top GPUs (n=57)</td>
      <td>-</td>
      <td>2.95 years[2.54, 3.52]</td>
      <td>9.81 years[8.45, 11.71]</td>
      <td>FLOP/s per dollar</td>
    </tr>
    <tr style="background-color: #b7b7b7">
      <td>Top GPUs (n=57)</td>
      <td>-</td>
      <td>2.69 years[2.40, &nbsp;3.30]</td>
      <td>8.92 years[7.99, 10.95]</td>
      <td>FLOP/s</td>
    </tr>
  </tbody>
  <caption>
  Table 3. Summary of our findings on GPU price-performance trends and relevant trends in the existing literature. 95% confidence intervals are displayed in square brackets.
  </caption>
</table>

<figure>
  <img style="max-width: calc(min(100%, 544px));" src="/assets/images/posts/2022/gpu-perf/image3.png" />
</figure>
<figcaption class="caption">
  <p>Figure 8. FLOP/s per dollar for our dataset and various subgroups compared to relevant trends found elsewhere</p>
</figcaption>

<h1 id="conclusion">Conclusion</h1>

<p>We find that the trend of all data shows a doubling time of 2.46 years, the trend implied by GPUs used in ML shows a doubling time of 2.07 years and the trend implied by every month’s top GPU shows a doubling time of 2.95. We think that a doubling time below 2 years or above 3 years is implausible given the data. Furthermore, we think that part of the trend in ML GPUs can be explained by ML prioritizing better GPUs rather than actual hardware advances. We, therefore, think that a doubling time of 2 years is too aggressive and ~2.5 years accurately describes the doubling time of price-performance for GPUs over the past 15 years.</p>

<h1 id="appendix-a---dropping-data-before-2006">Appendix A - dropping data before 2006</h1>

<p>On balance, we felt like the arguments for keeping the data were weaker than for removing them. In short, it’s very unclear whether pre-2006 data are measured in a comparable way and whether pre-2006 GPUs are even really comparable to post-2006 GPUs.</p>

<p>Arguments for including pre-2006 data:</p>

<ol>
  <li>The median group provides the data and somehow got a hold of the estimated FLOP/s</li>
</ol>

<p>Arguments against including pre-2006 data:</p>

<ol>
  <style>
    .no-flops { background-color: #f4cccc; }
    .yes-flops { background-color: #d9ead3; }
  </style>
  <li>The data just looks fishy in the graph</li>
  <li>Alyssa Vance pointed out in a comment that general-purpose GPUs have not been developed until 2005 and cuda didn’t exist until 2007. This means we are talking about some very different GPUs that have little to do with the GPUs we are talking about today.</li>
  <li>The measure of “theoretical performance” is unavailable for most pre-2006 GPUs. It is therefore unclear how this data was initially collected.
    <ol>
      <li>List of pre-2006 GPUs:
        <ol>
          <li><span class="no-flops">GeForce 2 GTS Pro (<a href="https://www.techpowerup.com/gpu-specs/geforce2-gts-pro.c1722">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce 3 (<a href="https://www.techpowerup.com/gpu-specs/geforce3.c738">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce 3 Ti500 (<a href="https://www.techpowerup.com/gpu-specs/geforce3-ti500.c741">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce 3 Ti200 (<a href="https://www.techpowerup.com/gpu-specs/geforce3-ti200.c742">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce FX 5200 (<a href="https://www.techpowerup.com/gpu-specs/geforce-fx-5200.c60">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce FX 5200 Ultra (<a href="https://www.techpowerup.com/gpu-specs/geforce-fx-5200-ultra.c61">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce FX 5600 Ultra (<a href="https://www.techpowerup.com/gpu-specs/geforce-fx-5600-ultra.c65">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce FX 5800 (<a href="https://www.techpowerup.com/gpu-specs/geforce-fx-5800.c703">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce FX 5800 Ultra (<a href="https://www.techpowerup.com/gpu-specs/geforce-fx-5800-ultra.c74">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce FX 5900 / 5900 XT / 5900 ZT (<a href="https://www.techpowerup.com/gpu-specs/geforce-fx-5900-zt.c75">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce FX 5700 Ultra (<a href="https://www.techpowerup.com/gpu-specs/geforce-fx-5700-ultra.c70">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce FX 5950 Ultra (<a href="https://www.techpowerup.com/gpu-specs/geforce-fx-5950-ultra.c79">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce FX 5900 Ultra (<a href="https://www.techpowerup.com/gpu-specs/geforce-fx-5900-ultra.c704">no FLOPs</a>)</span></li>
          <li><span class="no-flops">GeForce 6200 TurboCache 64-TC/256MB (<a href="https://www.techpowerup.com/gpu-specs/geforce-6200-turbocache.c909">no FLOPs</a>)</span></li>
          <li><span class="yes-flops">ATI Xbox 360 GPU 90nm (<a href="https://www.techpowerup.com/gpu-specs/xbox-360-gpu-90nm.c1919#:~:text=The%20Xbox%20360%20GPU%2090nm,device%20supports%20DirectX%209.0c.">YES FLOPs</a>)</span></li>
        </ol>
      </li>
      <li>After 2006 it looks like the GPU model cards contain this FP32 FLOPs performance. Here are some samples
        <ol>
          <li><span class="no-flops">GeForce 7900 GX2 (2006, <a href="https://www.techpowerup.com/gpu-specs/geforce-7900-gx2.c172">no FLOPs</a>)</span></li>
          <li><span class="yes-flops">GeForce 8800 GTX (2006, <a href="https://www.techpowerup.com/gpu-specs/geforce-8800-gtx.c187">YES FLOPs</a>)</span></li>
          <li><span class="yes-flops">GeForce 8800 GTS 640 (2006, <a href="https://www.techpowerup.com/gpu-specs/geforce-8800-gts-640.c757">YES FLOPs</a>)</span></li>
          <li><span class="yes-flops">GeForce 8600 GT (2007, <a href="https://www.techpowerup.com/gpu-specs/geforce-8600-gt.c198">YES FLOPs</a>)</span></li>
          <li><span class="yes-flops">GeForce 8500 GT (2007, <a href="https://www.techpowerup.com/gpu-specs/geforce-8500-gt.c765">YES FLOPs</a>)</span></li>
          <li><span class="yes-flops">Radeon HD 2900 XT (2007, <a href="https://www.techpowerup.com/gpu-specs/radeon-hd-2900-xt.c192">YES FLOPs</a>)</span></li>
          <li><span>Trend continues afterward</span></li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<h1 id="appendix-b---robustness-check-for-flops">Appendix B - Robustness check for FLOP/s</h1>

<p>In our dataset, we only look at GPUs for which we have the FLOP/s and price information since we are interested in performance and price. However, there are many more GPUs that have performance information than ones for which we have both performance and price. We find 1848 data points for which we have FLOP/s data. To make sure that there is no selection effect, we also analyze the trend from “just FLOP/s”.</p>

<figure>
  <img src="/assets/images/posts/2022/gpu-perf/image13.png" />
  <figcaption class="caption">
    <p>Figure 9. Empirical FLOP/s with all GPUs that we have FLOP/s information for</p>
  </figcaption>
</figure>

<p>We find that it pretty much aligns exactly with what we see from our previous selection and therefore preliminary conclude that there is no reason to discard our previous findings. We additionally see that the GPUs for which we have price data tend to be the ones with higher FLOP/s values. We speculate that more powerful GPUs are used more often and therefore have higher availability of price information.</p>

<h3 id="more-flops-plots">More FLOP/s plots</h3>

<p>For all the plots used in the paper, there is also a version in which we only look at FLOP/s information. Note, that this is not the “just-FLOP/s” data from the previous section. Rather it is the same dataset as used in the main text but we didn’t divide it by price.</p>

<div class="figure-flexbox">
  <figure>
    <img src="/assets/images/posts/2022/gpu-perf/image6.png" />
    <figcaption class="caption">
    Figure 10. Empirical FLOP/s for our dataset
    </figcaption>
  </figure>

  <figure>
    <img src="/assets/images/posts/2022/gpu-perf/image7.png" />
    <figcaption class="caption">
    Figure 11. Empirical FLOP/s for our dataset with subset of GPUs used for ML
    </figcaption>
  </figure>

  <figure>
    <img src="/assets/images/posts/2022/gpu-perf/image8.png" />
    <figcaption class="caption">
    Figure 12. Empirical FLOP/s for the GPUs with the highest FLOP/s value for every month
    </figcaption>
  </figure>

  <figure>
    <img src="/assets/images/posts/2022/gpu-perf/image2.png" />
    <figcaption class="caption">
    Figure 13. Empirical FLOP/s for top FLOP GPUs and ML GPUs combined
    </figcaption>
  </figure>
</div>

<p><br /></p>

<figure>
  <img src="/assets/images/posts/2022/gpu-perf/image4.png" />
  <figcaption class="caption">
    <p>Figure 14. Empirical FLOPs for FP16 and FP32</p>
  </figcaption>
</figure>

<p>We did not include these in the main text because they show very similar slopes as the FLOPs/dollar and we think FLOPs/dollar is the more important metric.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:a" role="doc-endnote">
      <p>This work <em>does not</em> attempt to project future GPU price-performance but merely to take stock of the recent historical trend. In future work, we intend to investigate what GPU price-performance trends informs us about historical growth in hardware spending in Machine Learning and future large Machine Learning training runs. <a href="#fnref:a" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:b" role="doc-endnote">
      <p>Moreover, we had discovered issues in estimates from a prior investigation by the <a href="http://mediangroup.org/gpu.html">Median Group (2018)</a>, which we have since pointed out to them, and which they have since corrected. <a href="#fnref:b" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p>We unfortunately can’t publish our data but the code that generated all the figures can be found <a href="https://colab.research.google.com/drive/1wGoqa1ErAzuzDZ6hgLrhubS6YHWNAp9I?usp%3Dsharing">here</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>For example, consider NVIDIA Tesla K40c and NVIDIA Tesla K40d to be the same models, as these have essentially identical specifications. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>We focus primarily on these metrics as we are mostly interested in questions related to the amount of compute that might be deployed for large AI experiments. While there are other metrics that might be of interest (such as energy efficiency), we do not consider these here as they relate less directly to the questions motivating our work, and because these have been analyzed in prior work, notably in <a href="https://arxiv.org/pdf/1911.11313.pdf">Sun et al., 2019</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>This is our interpretation of section 4 of her draft report, where she writes “I also assume that effective FLOP/s per dollar is doubling roughly once every 2.5 years around 2025. This is slower than Moore’s law (which posits a ~1-2 year doubling time and described growth reasonably well until the mid-2000s) but faster than growth in effective FLOP/s per dollar from ~2008 to 2018 (a doubling time of ~3-4 years)” <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>FLOP per dollar in <a href="https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">Cotra 2020</a> refers to the total amount of computation that can be done per dollar. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>We also took an initial look at FP64 performance but decided not to include the analysis because FP64 performance seems to be much lower than FP32 performance for newer GPUs. We interpret this as GPU companies deprioritizing FP64 in favor of FP32 and FP16. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>This is, we suspect, due to the fact that their approach involves analyzing the moving optima—which, in their case, involves analyzing 9 data points, which we think is insufficient to yield confidence in their point estimate. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
<hr>

              <p class="hiring-message">
                We are hiring! If you want to contribute to our research, consider applying to one of <a href="/careers#open-positions">our job offers</a>.
              </p></d-article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <span>© 2022 Epoch. Epoch is a fiscal sponsorship project of <a href="https://rethinkpriorities.org/">Rethink Priorities</a>.</span>

  <div class="footer-icons-container">
    <div class="footer-icons">
      <a data-contact="aW5mb0BlcG9jaGFpLm9yZw==" href="#">
        <i class="bi bi-envelope"></i>
      </a>
      <a href="https://github.com/epoch-research">
          <i class="bi bi-github"></i>
      </a>
      <a href="https://twitter.com/EpochAIResearch">
          <i class="bi bi-twitter"></i>
      </a>
    </div>
  </div>

</footer>
</body>

  <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <script src="/assets/js/footnotes.js"></script>
</html>

<script>
  // As seen here: https://www.bram.us/2020/01/10/smooth-scrolling-sticky-scrollspy-navigation/

  window.addEventListener('DOMContentLoaded', () => {
    // Smooth scrolling

    if (window.getComputedStyle(document.documentElement).scrollBehavior !== 'smooth') {
      for (let internalLink of document.querySelectorAll('a[href^="#"]')) {
        let href = internalLink.getAttribute('href');
        if (href == '#') continue;
        const targetElement = document.querySelector(href.replaceAll(':', '\\:'));
        if (targetElement) {
          internalLink.addEventListener('click', (e) => {
            targetElement.scrollIntoView({
              behavior: 'smooth',
            });
            e.preventDefault();
          });
        }
      };
    }

    // Highlight TOC on scroll

    let headers = [];
    for (let header of document.querySelectorAll('h1[id]')) {
      let subHeaders = [];
      for (let subHeader of document.querySelectorAll('h2[id]')) {
        subHeaders.push(subHeader);
      }
      headers.push[{
        dom: header,
        subHeaders: subHeaders,
      }];
    }

    document.addEventListener('scroll', function(e) {
      let currentHeader;

      let headers = document.querySelectorAll('h1[id], h2[id], h3[id]');
      let middleY = document.documentElement.clientHeight/2;
      for (let i = 0; i < headers.length; i++) {
        let p0 = headers[i].getBoundingClientRect().top;
        let p1 = (i < headers.length - 1) ? headers[i+1].getBoundingClientRect().top : 999999;

        if (p0 <= middleY && p1 > middleY) {
          currentHeader = headers[i];
          break;
        }
      }

      if (currentHeader) {
        let id = currentHeader.id;
        document.querySelectorAll(`d-contents nav li a:not([href="#${id}"])`).forEach(node => node.classList.remove('active'));
        document.querySelector(`d-contents nav li a[href="#${id}"]`).classList.add('active');
      } else {
        document.querySelectorAll(`d-contents nav li a`).forEach(node => node.classList.remove('active'));
      }
    });
  });
</script>
