<!DOCTYPE html>




<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Parameter Counts in Machine Learning" />
<meta name="author" content="Jaime Sevilla" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Compiling a large dataset of Machine Learning models to determine changes in the parameters counts of systems since 1952." />
<meta property="og:description" content="Compiling a large dataset of Machine Learning models to determine changes in the parameters counts of systems since 1952." />
<link rel="canonical" href="https://epoch-website-dev.web.app/blog/parameter-counts" />
<meta property="og:url" content="https://epoch-website-dev.web.app/blog/parameter-counts" />
<meta property="og:site_name" content="Epoch" />
<meta property="og:image" content="https://epoch-website-dev.web.app/assets/images/posts/2022/parameter-counts.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-19T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://epoch-website-dev.web.app/assets/images/posts/2022/parameter-counts.png" />
<meta property="twitter:title" content="Parameter Counts in Machine Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jaime Sevilla"},"dateModified":"2021-06-19T00:00:00+00:00","datePublished":"2021-06-19T00:00:00+00:00","description":"Compiling a large dataset of Machine Learning models to determine changes in the parameters counts of systems since 1952.","headline":"Parameter Counts in Machine Learning","image":"https://epoch-website-dev.web.app/assets/images/posts/2022/parameter-counts.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://epoch-website-dev.web.app/blog/parameter-counts"},"url":"https://epoch-website-dev.web.app/blog/parameter-counts"}</script>
<!-- End Jekyll SEO tag -->

  <title> Parameter Counts in Machine Learning </title>

  
    






  

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css">
  <link rel="stylesheet" href="/assets/css/micromodal.css">
  <link rel="icon" type="image/svg+xml" href="/assets/images/favicon.svg" >
  <link rel="shortcut icon" type="image/png" href="/assets/images/favicon.png" ><!-- MathJax -->
  
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true,
      },

      options: {
        ignoreHtmlClass: 'tex2jax_ignore',
      },
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
  

  <link rel="stylesheet" href="/assets/css/main.css">

  <script src="/assets/js/micromodal.min.js"></script>

  <!-- Copy buttons -->
  <script src="/assets/js/clipboard.min.js"></script>
  <script>
  // TODO Temporary, compress
  function addCopyButton(node) {
    let copyButton;

    if (node.tagName == 'PRE') {
      copyButton = document.createElement('button');
      node.parentElement.appendChild(copyButton);
      node.parentElement.classList.add('copiable-wrapper');
      node.classList.add('copy-target');
      copyButton.classList.add('copy-button');
      copyButton.innerHTML = '<i class="bi-clipboard"></i>';
    } else {
      let wrapper = document.createElement('div');
      wrapper.classList.add('copiable-wrapper');
      node.parentNode.insertBefore(wrapper, node);
      wrapper.appendChild(node);

      copyButton = document.createElement('button');
      node.parentElement.appendChild(copyButton);
      node.parentElement.classList.add('copiable-wrapper');
      node.classList.add('copy-target');
      copyButton.classList.add('copy-button');
      copyButton.innerHTML = '<i class="bi-clipboard"></i>';
    }

    copyButton.querySelector('i').style.backgroundColor = node.style.backgroundColor;

    let clipboard = new ClipboardJS(copyButton, {
      target: function(trigger) {
        return trigger.parentElement.querySelector('.copy-target');
      },
    });

    let tooltip = tippy(copyButton, {
      content: 'Copied',
      trigger: 'manual',
      placement: 'left',
      appendTo: copyButton,
      arrow: false,
      offset: [1, -1],
    });

    clipboard.on('success', function(e) {
      e.clearSelection();

      let icon = e.trigger.querySelector('i');
      icon.classList.remove('bi-clipboard');
      icon.classList.add('bi-clipboard-check');

      tooltip.show();
      setTimeout(() => {
        tooltip.hide();
        icon.classList.remove('bi-clipboard-check');
        icon.classList.add('bi-clipboard');
      }, 1200);
    });

    clipboard.on('error', function(e) {
      e.clearSelection();
    });
  }

  window.addEventListener('DOMContentLoaded', () => {
    for (let element of document.querySelectorAll('pre, .boxed-text')) {
      addCopyButton(element);
    }
  });
</script>

</head>
<head>
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>

    
      

<script src="https://unpkg.com/@popperjs/core@2.11.2"></script>
<script src="https://unpkg.com/tippy.js@6.2.6"></script>



<style>
  :root {
    --resource-color: 78, 51, 80;
  }

  d-title {
    padding: 0;
  }

  d-title {
    margin-bottom: 15px;
  }

  d-article {
    border-top: 0;
    padding-top: 0;
  }

  .article-head {
    width: 100%;
    background-color: var(--header-color);
    margin-bottom: 4em;
  }

  .article-head-content {
    padding-left: var(--nav-bar-margin);
    padding-right: var(--nav-bar-margin);
    display: flex;
    margin: auto;
    padding-top: 70px;
    padding-bottom: 20px;
    box-sizing: border-box;
  }

  .page-content {
    padding-top: 0;
  }

  .summary-supertitle {
    margin-bottom: 0;
    margin-top: 10px;
    font-size: 0.7rem;
    text-transform: uppercase;
    word-spacing: 3px;
  }

  .summary-title {
    margin-bottom: 15px;
  }

  .summary-title h1 {
    margin-bottom: 0;
  }

  .cite-us {
    background-color: transparent;
    cursor: pointer;
    border: 0;
    font-size: 0.9rem;
    color: white !important;
  }

  .citation-tooltip {
    max-width: calc(100vw - 27px);
    box-shadow: 0 0 7px 7px black;
    border-radius: var(--default-radius);
  }

  .citation-tooltip > .tippy-box {
    padding-top: 0.5em;
  }

  .citation-tooltip pre * {
    color: var(--code-color);
  }

  @media (min-width: 800px) {
    .regular-banner .article-head-content {
      width: auto;
      height: calc(50vh);
      padding-top: 0;
      padding-bottom: 1em;
      padding-left: var(--nav-bar-margin);
      padding-right: var(--nav-bar-margin);
    }

    .regular-banner .banner-img-wrapper {
      width: 100%;
      height: 100%;
    }

    .regular-banner .banner-img-wrapper img {
      width: 100%;
      height: 100%;
      object-fit: cover;
      object-position: left;
    }

    .regular-banner .summary {
      flex: 0 0 50%;
      box-sizing: border-box;
    }

    .regular-banner .summary-main {
      width: 90%;
      margin: auto;
      margin-left: 0;
    }

    .regular-banner .banner {
      flex: 0 0 50%;
      margin: 0;
      padding: 0;
      object-fit: cover;
    }
}

  

  .summary {
    order: 1;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    padding-right: 1em;
  }

  .summary-title, .summary-title *,
  .summary-authors, .summary-authors *,
  .summary-abstract, .summary-abstract *,
  .summary-supertitle, .summary-supertitle *,
  .summary-footer, .summary-footer * {
    color: white;
  }

  .article-resources, .article-resources * {
    color: white !important;
  }

  .summary .copiable-wrapper i {
    color: black;
  }

  .summary-footer {
    margin-top: 2rem;
    font-size: 0.9rem;
  }

  .summary-authors {
    font-size: 0.9rem;
    margin-bottom: 1rem;
  }

  .banner {
    order: 2;
    flex: 0 0 41%;
    margin-left: auto;
    margin-right: auto;
    padding-left: 10px;
    //padding-right: 10px;
  }

  .banner-img-wrapper img {
    border-radius: var(--default-radius);
  }

  .article-resources {
    margin-top: 2rem;
    display: flex;
    flex-wrap: wrap;
  }

  .article-resource {
    border-radius: 4px;
    background-color: rgba(var(--resource-color), 0.8);
    padding: 6px 11px;
    font-size: 0.8rem;
    font-weight: bold;
    text-transform: uppercase;
    margin: 5px;
    margin-top: 0.5em;
    white-space: nowrap;
  }

  .article-resources i {
    margin-right: 0.5rem;
  }

  .article-resource:hover {
    background-color: rgba(var(--resource-color), 1.0);
  }

  @media (max-width: 800px) {
    .article-head-content {
      flex-wrap: wrap;
    }

    .banner, .regular-banner .banner {
      order: 1;
      flex-basis: 100%;
      margin-left: 0;
      margin-right: 0;
      padding-left: 0;
      text-align: left;
    }

    .summary {
      order: 2;
    }
  }
</style>

    
  </head>

  <body><header class="site-header" role="banner">
  <div class="header-wrapper"><a href="/">
      <img src="/assets/images/epoch-logo-white-text.svg" alt="Epoch logo" class="header-logo">
      
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
          
          <a class="page-link" href="/research">Research</a>
          <a class="page-link current-menu-item" href="/blog">Blog</a>
          <a class="page-link" href="/mlinputs/visualization">Visualization</a>
          <a class="page-link" href="/team">Team</a>
          <a class="page-link" href="/careers">Careers</a>
        </div>
      </nav></div>

  <script>
    document.addEventListener('click', (e) => {
      let navTrigger = document.querySelector('#nav-trigger');
      if (navTrigger && navTrigger.checked) {
        if (e.target != navTrigger.parentElement && !navTrigger.parentElement.contains(e.target)) {
          navTrigger.checked = false;
        }
      }
    });
  </script>

  <style>
    .image-tooltip {
      display: inline-block;
      width: 400px !important;
    }

    .image-tooltip img {
      width: 100%;
    }
  </style>

  <script>
    window.addEventListener('load', () => {
      for (let tooltipedElement of document.querySelectorAll('[data-tooltip-image]')) {
        let href = tooltipedElement.href;
        let imageUrl = tooltipedElement.dataset.tooltipImage;

        if (!imageUrl) continue;

        let content;
        if (href) {
          content = `<a class="image-tooltip" href="${href}"><img src='${imageUrl}'></img></a>`;
        } else {
          content = `<img class="image-tooltip" src='${imageUrl}'></img>`;
        }

        tippy(tooltipedElement, {
          allowHTML: true,
          placement: 'top',
          arrow: false,
          interactive: true,
          maxWidth: '500px',
          trigger: 'mouseenter',
          onShow: () => {
            tippy.hideAll();
          },
          content: content,
        });
      }
    });
  </script>

  <script>
    // Deobfuscate email addresses

    function addMailDeobfuscator(element) {
      element.addEventListener('mouseover', () => element.href = 'mailto:' + atob(element.dataset.contact));
      element.addEventListener('focus', () => element.href = 'mailto:' + atob(element.dataset.contact));
    }

    window.addEventListener('load', () => {
      for (let element of document.querySelectorAll("[data-contact]")) {
        addMailDeobfuscator(element);
      }
    });
  </script>
</header>
<main class="page-content" aria-label="Content">
      <div class="post distill">

        
          

<div class="article-head ">
  <div class="article-head-content">
    <div class="banner">
      <div class="banner-img-wrapper">
        <img src="/assets/images/posts/2022/parameter-counts.png"/>
      </div>
    </div>
    <div class="summary">
      <div class="summary-main">
        
        <div class="summary-title">
          <h1>Parameter Counts in Machine Learning</h1>
          <a class="cite-us"><i class="bi-journal-text"></i> Cite this post</a>
        </div>
        
        <div class="summary-authors">
          
          



      
      <span class="author tooltiped" data-member-id="jaime-sevilla">Jaime Sevilla</span>, 
      
      <span class="author tooltiped" data-member-id="pablo-villalobos">Pablo Villalobos</span> and <span class="author">Juan Felipe Cer√≥n</span>

<script src="/assets/js/umbrella.min.js"></script>
<style>
  .author.tooltiped {
    cursor: pointer;
  }

  .summary-authors .tippy-content {
    width: 180px;
    padding: 5px;
  }

  .miniprofile .mug {
    width: 180px;
    height: 180px;
    background-size: cover;
    background-position: center;
  }

  .miniprofile a {
    color: black;
    text-decoration: none;
  }

  .miniprofile .member-resource {
    margin-right: 5px;
  }

  .miniprofile .member-info {
    padding: 4px;
    width: 180px;
  }

  .miniprofile .member-name, .miniprofile .member-role {
    margin-bottom: 2px;
  }
</style>

<script>

  

  

  let teamMembers = {
    
      'jaime-sevilla': {
        id: 'jaime-sevilla',
        name: 'Jaime Sevilla',
        description: 'Jaime is a researcher focused on statistics and technological forecasting. Besides his role at Epoch, he is a research affiliate of the <a href="https://www.cser.ac.uk/">Centre for the Study of Existential Risk</a> at Cambridge University and the cofounder of <a href="https://riesgoscatastroficosglobales.com/">Riesgos Catastr√≥ficos Globales</a>.',
        role: 'Director',
        imageUrl: '/assets/images/team/jaime-sevilla.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'amFpbWVAZXBvY2hhaS5vcmc='},
        
          
          
          
          
          
          {name: 'twitter', icon: 'twitter', url: 'https://twitter.com/Jsevillamol'},
        
        ],
      },
    
      'tamay-besiroglu': {
        id: 'tamay-besiroglu',
        name: 'Tamay Besiroglu',
        description: 'Tamay is a researcher focusing on the Economics of Computing and big-picture trends in Machine Learning. In addition to his role at Epoch, Tamay is a researcher at the Future Tech Lab at MIT, and AI Forecasting Lead at Metaculus. Previously, he led strategy for Metaculus, consulted for the UK Government, and worked at the Future of Humanity Institute.',
        role: 'Associate director',
        imageUrl: '/assets/images/team/tamay-besiroglu.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'dGFtYXlAZXBvY2hhaS5vcmc='},
        
        ],
      },
    
      'pablo-villalobos': {
        id: 'pablo-villalobos',
        name: 'Pablo Villalobos',
        description: 'Pablo has a background in Mathematics and Computer Science. After spending some time as a software engineer, he decided to pivot towards AI. His interests include the economic consequences of advanced AI systems and the role of algorithmic improvements in AI progress.',
        role: 'Staff Researcher',
        imageUrl: '/assets/images/team/pablo-villalobos.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'cGFibG9AZXBvY2hhaS5vcmc='},
        
        ],
      },
    
      'anson-ho': {
        id: 'anson-ho',
        name: 'Anson Ho',
        description: 'Anson is a researcher and writer for Epoch. His research interests are in interpretability, theoretical AI alignment, and ensuring safe AI development through governance and strategy. Prior to this, he completed his BSc in physics at the University of St Andrews.',
        role: 'Staff Researcher',
        imageUrl: '/assets/images/team/anson-ho.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'YW5zb25AZXBvY2hhaS5vcmc='},
        
          
          
          
          
          
          {name: 'github', icon: 'github', url: 'https://github.com/ansonwhho'},
        
          
          
          
          
          
          {name: 'website', icon: 'globe', url: 'https://ansonwhho.github.io/'},
        
        ],
      },
    
      'lennart-heim': {
        id: 'lennart-heim',
        name: 'Lennart Heim',
        description: 'Lennart is a researcher on AI and compute. His research interests include the role of compute in the AI production function, the compute landscape/supply chain, security of AI systems, and forecasting emerging technologies. He is a research affiliate with the Centre for the Governance of AI in Oxford and has a background in Computer Engineering.',
        role: 'Research Fellow and Strategy Specialist',
        imageUrl: '/assets/images/team/lennart-heim.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'bGVubmFydEBlcG9jaGFpLm9yZw=='},
        
          
          
          
          
          
          {name: 'twitter', icon: 'twitter', url: 'https://twitter.com/ohlennart'},
        
          
          
          
          
          
          {name: 'website', icon: 'globe', url: 'https://heim.xyz'},
        
        ],
      },
    
      'marius-hobbhahn': {
        id: 'marius-hobbhahn',
        name: 'Marius Hobbhahn',
        description: 'Marius builds models for AI timelines and takeoff using historical trends and his best understanding of the future.',
        role: 'Research Fellow',
        imageUrl: '/assets/images/team/marius-hobbhahn.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'bWFyaXVzQGVwb2NoYWkub3Jn'},
        
        ],
      },
    
      'eduardo-infante-roldan': {
        id: 'eduardo-infante-roldan',
        name: 'Eduardo Infante-Rold√°n',
        description: 'Eduardo does some programming.',
        role: 'Software Engineer',
        imageUrl: '/assets/images/team/eduardo-infante-roldan.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'ZWR1QGVwb2NoYWkub3Jn'},
        
        ],
      },
    
      'ege-erdil': {
        id: 'ege-erdil',
        name: 'Ege Erdil',
        description: 'Ege Erdil is an undergraduate student at Middle East Technical University. He has interests in mathematics, statistics, economics and forecasting.',
        role: 'Intern Researcher',
        imageUrl: '/assets/images/team/ege-erdil.jpg',
        resources: [
        
          
          
          
          
          
          {name: 'mail', icon: 'envelope', url: 'ZWdlQGVwb2NoYWkub3Jn'},
        
        ],
      },
    
  };

  

  

  let advisors = {
    
      'tom-davidson': {
        id: 'tom-davidson',
        name: 'Tom Davidson',
        description: 'Tom is a senior research analyst at Open Philanthropy. He‚Äôs currently working on assessing arguments that transformative AI might be developed relatively soon. Prior to joining Open Philanthropy, Tom worked as a Data Scientist for education technology startup BridgeU and taught science at a UK comprehensive school. He has a Masters in Physics and Philosophy from the University of Oxford.',
        role: 'Research Advisor',
        imageUrl: '/assets/images/advisors/tom-davidson.jpg',
        resources: [
        
        ],
      },
    
      'neil-thompson': {
        id: 'neil-thompson',
        name: 'Neil Thompson',
        description: 'Neil is an Innovation Scholar at MIT‚Äôs Computer Science and Artificial Intelligence Lab and the Initiative on the Digital Economy where he leads the FutureTech Project. He is also an Associate Member of the Broad Institute.',
        role: 'Research Advisor',
        imageUrl: '/assets/images/advisors/neil-thompson.jpg',
        resources: [
        
        ],
      },
    
  };

  

  let members = {...teamMembers, ...advisors};
</script>


<script>
  for (let author of document.querySelectorAll('.author.tooltiped')) {
    let member = teamMembers[author.dataset.memberId];
    if (!member) continue;

    let resources = [];
    for (let resource of member.resources) {
      if (resource.name == "mail") {
        resources.push(`<a class="member-resource" href="#" data-contact="${resource.url}"><i class="bi bi-${resource.icon}"></i></a>`);
      } else {
        resources.push(`<a class="member-resource" href="${resource.url}"><i class="bi bi-${resource.icon}"></i></a>`);
      }
    }

    tippy(author, {
      allowHTML: true,
      placement: 'top',
      arrow: false,
      interactive: true,
      maxWidth: '200px',
      trigger: 'mouseenter click',
      onShow: (instance) => {
        tippy.hideAll();
        for (let element of instance.popper.querySelectorAll('[data-contact]')) {
          addMailDeobfuscator(element);
        }
      },
      content:
        `
        <div class="miniprofile">
          <div class="mug" style="border-radius: 5px; background-image: url('${member.imageUrl}')"></div>
          <div class="member-info">
            <h3 class="member-name">${member.name}</h3>
            <h4 class="member-role">${member.role}</h4>
            <div class="member-resources">
              ${resources.join('\n')}
            </div>
          </div>
        </div>
        `,
    });
  }
</script>

        </div>
        
        
        <div class="summary-abstract">
          Compiling a large dataset of Machine Learning models to determine changes in the parameters counts of systems since 1952.
        </div>
        
        
          <div class="article-resources">
            
            <a class="article-resource" href="/mlinputs/visualization"><i class="bi bi-graph-up"></i>Visualization</a>
            
            <a class="article-resource" href="/mlinputs/data"><i class="bi bi-table"></i>Dataset</a>
            
          </div>
        
      </div>
      
      <div class="summary-footer">
        Jun. 19, 2021
      </div>
      
    </div>
  </div>
</div>







<script>
  let content = `
    
<p style="margin-bottom: 0">Cite this work as</p>

<div class="language-plaintext highlighter-rouge wrappable-pre">
<div class="highlight">
  <pre class="highlight"><code>Jaime Sevilla, Pablo Villalobos and Juan Felipe Cer√≥n (2021), "Parameter Counts in Machine Learning". <em>Published online at epochai.org.</em> Retrieved from: 'https://epoch-website-dev.web.app/blog/parameter-counts' [online resource]</code></pre>
</div>
</div>

<p style="margin-bottom: 0">BibTeX citation:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{epoch2021parametercounts,
  title = "Parameter Counts in Machine Learning",
  author = {Jaime Sevilla, Pablo Villalobos and Juan Felipe Cer√≥n},
  year = 2021,
  howpublished = "\\url{https://epoch-website-dev.web.app/blog/parameter-counts}",
  note = "Accessed: 2022-07-19"
}
</code></pre></div></div>

  `;

  tippy('.cite-us', {
    allowHTML: true,
    placement: 'bottom-start',
    arrow: false,
    interactive: true,
    maxWidth: '900px',
    trigger: 'click',
    content: content,
    onShow: (instance) => {
      instance.popper.classList.add('citation-tooltip');
      tippy.hideAll();
      instance.popper.querySelectorAll('pre').forEach(pre => addCopyButton(pre));
    },
  });
</script>


        

        <d-article>
          <d-contents>
            <nav class="l-text figcaption">
            <h3>Contents</h3><div><ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#features-of-the-dataset">Features of the dataset</a></li>
<li class="toc-entry toc-h1"><a href="#caveats">Caveats</a></li>
<li class="toc-entry toc-h1"><a href="#insights">Insights</a></li>
<li class="toc-entry toc-h1"><a href="#open-questions">Open questions</a></li>
<li class="toc-entry toc-h1"><a href="#next-steps">Next steps</a></li>
<li class="toc-entry toc-h1"><a href="#acknowledgements">Acknowledgements</a></li>
<li class="toc-entry toc-h1"><a href="#bibliography">Bibliography</a></li>
</ul></div></nav>
          </d-contents>

          <p><strong>In short:</strong> we have compiled information about the date of development and trainable parameter counts of n=139 machine learning systems between 1952 and 2021. This is, as far as we know, the biggest public dataset of its kind. You can access our dataset <a href="https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0">here</a>, and the code to produce an interactive visualization is available <a href="https://colab.research.google.com/drive/11m0AfSQnLiDijtE1fsIPqF-ipbTQcsFp?usp=sharing">here</a>.</p>

<p>We chose to focus on parameter count because previous work indicates that it is an important variable for model performance [<a href="#b:1">1</a>], because it helps as a proxy of model complexity and because it is information usually readily available or easily estimable from descriptions of model architecture.¬†</p>

<p>We hope our work will help AI researchers and forecasters understand one way in which models have become more complex over time, and ground their predictions of how the field will progress in the future. In particular, we hope this will help us tease apart how much of the progress in Machine Learning has been due to algorithmic improvements versus increases in model complexity.</p>

<p>It is hard to draw firm conclusions from our biased and noisy dataset. Nevertheless, our work seems to give weak support to two hypotheses:</p>

<ul>
  <li>There was no discontinuity in any domain in the trend of model size growth in 2011-2012. This suggests that the Deep Learning revolution was not due to an algorithmic improvement, but rather the point where the trend of improvement of Machine Learning methods caught up to the performance of other methods.</li>
  <li>In contrast, it seems there has been a discontinuity in model complexity for language models somewhere between 2016-2018. Returns to scale must have increased, and shifted the trajectory of growth from a doubling time of ~1.5 years to a doubling time of between 4 to 8 months.</li>
</ul>

<p>The structure of this article is as follows. We first describe our dataset. We point out some weaknesses of our dataset. We expand on these and other insights. We raise some open questions. We finally discuss some next steps and invite collaboration.</p>

<figure>
  <img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/afce9ae98a1eeac853fb148b12b41fc6162e0fd934bdbdf8.png" />
  <figcaption>
    <p>Model size of popular new Machine Learning systems between 1954 and 2021. Includes n=139 datapoints. See expanded and interactive version of this graph <a href="/mlinputs/visualization">here</a>.</p>
  </figcaption>
</figure>

<figure>
  <img src="https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c9f5006bfc9850bbc06a938271f2e17cd9056daca1eee9eb.png" />
  <figcaption>
    <p>Model size of popular new Machine Learning systems between 2000 and 2021. Includes n=114 datapoints. See expanded and interactive version of this graph <a href="/mlinputs/visualization">here</a>.</p>
  </figcaption>
</figure>

<h1 id="features-of-the-dataset">Features of the dataset</h1>

<ul>
  <li>The dataset spans systems from 1952 to 2020, though we included far more information about recent systems (from 2010 onwards).</li>
  <li>The systems we include encompass many types, including neural networks, statistical models, support vector machines, bayesian networks and other more exotic architectures. However we mostly included systems of the neural network kind.</li>
  <li>The systems are from many domains and were trained to solve many tasks. However we mostly focused on systems trained to solve vision, language and gaming tasks.</li>
  <li>We relied on a subjective criteria of notability to decide which systems to include. Our decisions were informed by citation counts (papers with more than 1000 citations), external validation (papers that received some kind of paper of the year award or similar) and historical importance (papers that were cited by other work as seminal). The references to this post include some overviews we used as a starting point to curate our dataset [<a href="#b:2">2-26</a>].</li>
  <li>Several models have versions at multiple scales. Whenever we encountered this in their original publication, we recorded whichever was presented in the paper as the main one, or the largest presented version. Sometimes we recorded multiple versions when we felt it was warranted, e.g. when multiple different versions were trained to solve different tasks.</li>
</ul>

<h1 id="caveats">Caveats</h1>

<ul>
  <li>It is important to take into account that model size is hardly the most important parameter to understand the progress of ML systems. Other arguably more important indicators of non-algorithmic progress in ML systems include training compute and training dataset size [<a href="#b:1">1</a>].</li>
  <li>Model size as a metric of model complexity is hardly comparable across domains or even architectures. For example, a mixture-of-expert model can achieve higher parameter counts but invest far less compute into training each parameter.</li>
  <li>Our selection of systems is biased in many important ways. We are biased towards academic publications (since information on commercial systems is harder to come by). We include more information about recent systems. We tended to include information about papers where the parameter counts were readily available, in particular larger models that were developed to test the limits of how large a model can be. We are biased towards papers published in English. We mostly focused on systems on vision, language and gaming tasks, while we have comparatively fewer papers on e.g. speech recognition, recommender systems or self driving. Lastly, we are biased towards systems we personally found interesting or impressive.</li>
  <li>Recollecting the information was a time consuming exercise that required us to read through hundreds of technical papers to gather the parameter counts. It is quite likely we have made some mistakes.</li>
</ul>

<h1 id="insights">Insights</h1>

<ul>
  <li>Unsurprisingly, there is an upward trend in model size. The trend seems exponential, and seems to have picked up its pace recently for language models. An eyeball estimate of the slope of progress suggests that the doubling rate was between 18 and 24 months from 2000 to 2016-2018 in all domains, and between 3 and 5 months from 2016-2018 onward in the language domain.</li>
  <li>The biggest models in terms of trainable parameters can be found in the language and recommender system domains. The biggest model we found was the 12 trillion parameter Deep Learning Recommender System from Facebook. We don‚Äôt have enough data on recommender systems to ascertain whether recommender systems have been historically large in terms of trainable parameters.</li>
  <li>Language models have been historically bigger than in other domains. This was because of statistical models whose parameterization scales with vocabulary size (e.g. as in the Hiero Machine Translation System from 2005) and word embeddings that also scale with vocabulary size (e.g. as in Word2Vec from 2013).</li>
  <li>Arguably Deep Learning started to proliferate in computer vision before it reached language processing (both circa 2011-2013), however the parameter counts of the second far surpass those of the first today. In particular, somewhere between 2016-2018 the trend of growth in language model size apparently greatly accelerated its pace, to a doubling time of between 4 and 8 months.</li>
  <li>Architectures on the game domain are small in terms of trainable parameters, below¬† vision architectures while apparently growing at a similar rhythm. Naively we expected otherwise, since playing games seems more complicated. However, in hindsight, what determines model size is what are the returns to scale; in more complex domains we should expect lower effective model sizes, as the models are more constrained in other ways.</li>
  <li>The trend of growth in model size has been relatively stable through the transition into the deep learning era in 2011-2012 in all domains we studied (though it is hard to say with certainty given the amount of data). This suggests that the deep learning revolution was less of a paradigm change and more of a natural continuation of existing tendencies, which finally surpassed other non-machine learning methods.</li>
</ul>

<h1 id="open-questions">Open questions</h1>

<ul>
  <li>Why is there a discrepancy in the trainable parameters magnitude and trend of growth in e.g. vision systems versus e.g. language systems? Some hypotheses are that language architectures scale better with size, that vision models are more bottlenecked on training data, that vision models require more compute per parameter or that the language processing ML community is ahead in experiment with large scale models (e.g. because they have access to more compute and resources).</li>
  <li>What caused the explosive growth in the size of language models from 2018 onwards? Was it a purely social phenomena as people realized the advantages of larger models, was it enabled by the discovery of architectures that scaled better with size, compute and data (e.g. transformers?) or was it caused by something else entirely?</li>
  <li>Do the scaling laws of Machine Learning for pre-and-post-deep-learning actually differ significatively? So far model size seems to suggest otherwise, what about other metrics?</li>
  <li>How can we more accurately estimate the rates of growth for each domain and period? For how long will current rates of growth be sustained?</li>
</ul>

<h1 id="next-steps">Next steps</h1>

<ul class="tex2jax_ignore">
  <li>We are interested in collaborating with other researchers to grow this dataset to be more representative and correcting any mistakes. As an incentive, we will pay $5 per mistake found or system addition (up to $600 total among all submissions; please contact us if you want to contribute with a donation to increase the payment cap). You can send your submissions to jaimesevillamolina at gmail dot com, preferably in spreadsheet format.</li>
  <li>We are interested in including other information about the systems, most notably compute and training dataset size.</li>
  <li>We want to include more information on other domains, specially on recommender systems.</li>
  <li>We want to look harder for systematic reviews and other already curated datasets of AI systems.</li>
</ul>

<h1 id="acknowledgements">Acknowledgements</h1>

<p><em>This article was written by Jaime Sevilla, Pablo Villalobos and Juan Felipe Cer√≥n. Jaime‚Äôs work is supported by a Marie Curie grant of the NL4XAI Horizon 2020 program.</em></p>

<p><em>We thank Girish Sastry for advising us on the beginning of the project, the Spanish Effective Altruism community for creating a space to incubate projects such as this one, and Haydn Belfield, Pablo Moreno and Ehud Reiter for discussion and system submissions.</em></p>

<h1 id="bibliography">Bibliography</h1>

<ol>
  <li><span id="b:1"></span><em>Kaplan et al., ‚ÄúScaling Laws for Neural Language Models,‚Äù 08361.</em></li>
  <li><span id="b:2"></span><em>1.6 History of Reinforcement Learning</em>. (n.d.). Retrieved June 19, 2021, from <a href="http://incompleteideas.net/book/first/ebook/node12.html">http://incompleteideas.net/book/first/ebook/node12.html</a></li>
  <li><em>AI and Compute</em>. (n.d.). Retrieved June 19, 2021, from <a href="https://openai.com/blog/ai-and-compute/">https://openai.com/blog/ai-and-compute/</a></li>
  <li><em>AI and Efficiency</em>. (2020, May 5). OpenAI. <a href="https://openai.com/blog/ai-and-efficiency/">https://openai.com/blog/ai-and-efficiency/</a></li>
  <li><em>AI Progress Measurement</em>. (2017, June 12). Electronic Frontier Foundation. <a href="https://www.eff.org/ai/metrics">https://www.eff.org/ai/metrics</a></li>
  <li><em>Announcement of the 2020 ACL Test-of-Time Awards (ToT) | ACL Member Portal</em>. (n.d.). Retrieved June 19, 2021, from <a href="https://www.aclweb.org/portal/content/announcement-2020-acl-test-time-awards-tot#:~:text=Each%20year%2C%20the%20ACL%20Test,papers%20from%2010%20years%20earlier.&amp;text=The%20winners%20were%20announced%20at%20ACL%202020.">https://www.aclweb.org/portal/content/announcement-2020-acl-test-time-awards-tot#:~:text=Each%20year%2C%20the%20ACL%20Test,papers%20from%2010%20years%20earlier.&amp;text=The%20winners%20were%20announced%20at%20ACL%202020.</a></li>
  <li>Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú. <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 610‚Äì623. <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a></li>
  <li><em>Best paper awards‚ÄîACL Wiki</em>. (n.d.). Retrieved June 19, 2021, from <a href="https://aclweb.org/aclwiki/Best_paper_awards">https://aclweb.org/aclwiki/Best_paper_awards</a></li>
  <li><em>bnlearn‚ÄîBayesian Network Repository</em>. (n.d.). Retrieved June 19, 2021, from <a href="https://www.bnlearn.com/bnrepository/">https://www.bnlearn.com/bnrepository/</a></li>
  <li><em>Brian Christian on the alignment problem</em>. (n.d.). 80,000 Hours. Retrieved June 19, 2021, from <a href="https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/">https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/</a></li>
  <li><em>Computer Vision Awards ‚Äì The Computer Vision Foundation</em>. (n.d.). Retrieved June 19, 2021, from <a href="https://www.thecvf.com/?page_id=413">https://www.thecvf.com/?page_id=413</a></li>
  <li>DARPA Grand Challenge. (2021). In <em>Wikipedia</em>. <a href="https://en.wikipedia.org/w/index.php?title=DARPA_Grand_Challenge&amp;oldid=1021627196">https://en.wikipedia.org/w/index.php?title=DARPA_Grand_Challenge&amp;oldid=1021627196</a></li>
  <li>Karim, R. (2020, November 28). <em>Illustrated: 10 CNN Architectures</em>. Medium. <a href="https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d">https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d</a></li>
  <li>Mohammad, S. M. (2020). Examining Citations of Natural Language Processing Literature. <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, 5199‚Äì5209. <a href="https://doi.org/10.18653/v1/2020.acl-main.464">https://doi.org/10.18653/v1/2020.acl-main.464</a></li>
  <li>Mudigere, D., Hao, Y., Huang, J., Tulloch, A., Sridharan, S., Liu, X., Ozdal, M., Nie, J., Park, J., Luo, L., Yang, J. A., Gao, L., Ivchenko, D., Basant, A., Hu, Y., Yang, J., Ardestani, E. K., Wang, X., Komuravelli, R., ‚Ä¶ Rao, V. (2021). High-performance, Distributed Training of Large-scale Deep Learning Recommendation Models. <em>ArXiv:2104.05158 [Cs]</em>. <a href="http://arxiv.org/abs/2104.05158">http://arxiv.org/abs/2104.05158</a></li>
  <li>Nilsson, N. (1974). Artificial Intelligence. <em>IFIP Congress</em>. <a href="https://doi.org/10.7551/mitpress/11723.003.0006">https://doi.org/10.7551/mitpress/11723.003.0006</a></li>
  <li>Posey, L. (2020, April 28). <em>History of AI Research</em>. Medium. <a href="https://towardsdatascience.com/history-of-ai-research-90a6cc8adc9c">https://towardsdatascience.com/history-of-ai-research-90a6cc8adc9c</a></li>
  <li>Raschka, S. (2019). A Brief Summary of the History of¬† Neural Networks and Deep Learning. <em>Deep Learning</em>, 29.</li>
  <li>Sanh, V., Debut, L., Chaumond, J., &amp; Wolf, T. (2020). DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. <em>ArXiv:1910.01108 [Cs]</em>. <a href="http://arxiv.org/abs/1910.01108">http://arxiv.org/abs/1910.01108</a></li>
  <li>Thompson, N. C., Greenewald, K., Lee, K., &amp; Manso, G. F. (2020). The Computational Limits of Deep Learning. <em>ArXiv:2007.05558 [Cs, Stat]</em>. <a href="http://arxiv.org/abs/2007.05558">http://arxiv.org/abs/2007.05558</a></li>
  <li>Vidal, R. (n.d.). <em>Computer Vision: History, the Rise of Deep Networks, and Future Vistas</em>. 60.</li>
  <li>Wang, B. (2021). <em>Kingoflolz/mesh-transformer-jax</em> [Jupyter Notebook]. <a href="https://github.com/kingoflolz/mesh-transformer-jax">https://github.com/kingoflolz/mesh-transformer-jax</a> (Original work published 2021)</li>
  <li><em>Who Invented Backpropagation?</em> (n.d.). Retrieved June 19, 2021, from <a href="https://people.idsia.ch//~juergen/who-invented-backpropagation.html">https://people.idsia.ch//~juergen/who-invented-backpropagation.html</a></li>
  <li>Xie, Q., Luong, M.-T., Hovy, E., &amp; Le, Q. V. (2020). Self-training with Noisy Student improves ImageNet classification. <em>ArXiv:1911.04252 [Cs, Stat]</em>. <a href="http://arxiv.org/abs/1911.04252">http://arxiv.org/abs/1911.04252</a></li>
  <li>Young, T., Hazarika, D., Poria, S., &amp; Cambria, E. (2018). Recent Trends in Deep Learning Based Natural Language Processing. <em>ArXiv:1708.02709 [Cs]</em>. <a href="http://arxiv.org/abs/1708.02709">http://arxiv.org/abs/1708.02709</a></li>
  <li>Zhang, B., Xiong, D., Su, J., Lin, Q., &amp; Zhang, H. (2018). Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks. <em>ArXiv:1810.12546 [Cs]</em>. <a href="http://arxiv.org/abs/1810.12546">http://arxiv.org/abs/1810.12546</a></li>
  <li>Zoph, B., &amp; Le, Q. V. (2016). <em>Neural Architecture Search with Reinforcement Learning</em>. <a href="https://arxiv.org/abs/1611.01578v2">https://arxiv.org/abs/1611.01578v2</a></li>
</ol>
<hr>

              <p class="hiring-message">
                We are hiring! If you want to contribute to our research, consider applying to one of <a href="/careers#open-positions">our job offers</a>.
              </p></d-article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <span>¬© 2022 Epoch. Epoch is a fiscal sponsorship project of <a href="https://rethinkpriorities.org/">Rethink Priorities</a>.</span>

  <div class="footer-icons-container">
    <div class="footer-icons">
      <a data-contact="aW5mb0BlcG9jaGFpLm9yZw==" href="#">
        <i class="bi bi-envelope"></i>
      </a>
      <a href="https://github.com/epoch-research">
          <i class="bi bi-github"></i>
      </a>
      <a href="https://twitter.com/EpochAIResearch">
          <i class="bi bi-twitter"></i>
      </a>
    </div>
  </div>

</footer>
</body>

  <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

  <script src="/assets/js/footnotes.js"></script>
</html>

<script>
  // As seen here: https://www.bram.us/2020/01/10/smooth-scrolling-sticky-scrollspy-navigation/

  window.addEventListener('DOMContentLoaded', () => {
    // Smooth scrolling

    if (window.getComputedStyle(document.documentElement).scrollBehavior !== 'smooth') {
      for (let internalLink of document.querySelectorAll('a[href^="#"]')) {
        let href = internalLink.getAttribute('href');
        if (href == '#') continue;
        const targetElement = document.querySelector(href.replaceAll(':', '\\:'));
        if (targetElement) {
          internalLink.addEventListener('click', (e) => {
            targetElement.scrollIntoView({
              behavior: 'smooth',
            });
            e.preventDefault();
          });
        }
      };
    }

    // Highlight TOC on scroll

    let headers = [];
    for (let header of document.querySelectorAll('h1[id]')) {
      let subHeaders = [];
      for (let subHeader of document.querySelectorAll('h2[id]')) {
        subHeaders.push(subHeader);
      }
      headers.push[{
        dom: header,
        subHeaders: subHeaders,
      }];
    }

    document.addEventListener('scroll', function(e) {
      let currentHeader;

      let headers = document.querySelectorAll('h1[id], h2[id], h3[id]');
      let middleY = document.documentElement.clientHeight/2;
      for (let i = 0; i < headers.length; i++) {
        let p0 = headers[i].getBoundingClientRect().top;
        let p1 = (i < headers.length - 1) ? headers[i+1].getBoundingClientRect().top : 999999;

        if (p0 <= middleY && p1 > middleY) {
          currentHeader = headers[i];
          break;
        }
      }

      if (currentHeader) {
        let id = currentHeader.id;
        document.querySelectorAll(`d-contents nav li a:not([href="#${id}"])`).forEach(node => node.classList.remove('active'));
        document.querySelector(`d-contents nav li a[href="#${id}"]`).classList.add('active');
      } else {
        document.querySelectorAll(`d-contents nav li a`).forEach(node => node.classList.remove('active'));
      }
    });
  });
</script>
