---
layout: article
title: "Announcing Epoch: A research initiative monitoring the road to transformative AI"
subtitle: Subtitle
image: assets/images/epoch-logo.svg
description: We're a new research initiative working on monitoring AI developments and forecasting the developments of AI. Come join us!
tags: announcements

banner:
  fullscreen: false

toc: auto

date: 2022-04-26

authors:
  - name: The Epoch Team

---

# Summary
- We are a new research initiative working on monitoring AI developments and forecasting the development of transformative AI (TAI)
- Our mission is to narrow the gap between technical AI developments and AI governance, and inform key decision makers in AI safety (e.g. grantmakers)
- This work is done in close collaboration with other organisations, like Rethink Priorities, and MIT CSAIL
- We will be hiring for 3-4 full-time roles between May and July – please register your interest here
- We plan to release full details about our research agenda, hiring, and organisation over the coming weeks

## What is Epoch?
Epoch is a new research initiative (previously called Machine Learning Progress) that works to support AI governance and improve forecasts around the development of transformative AI (TAI) – AI systems that have the potential to have an effect on society as large as that of the industrial revolution (Karnofsky, 2016).

Our team currently consists of seven members – Jaime Sevilla, Tamay Besiroglu, Lennart Heim, Pablo Villalobos, Eduardo Infante-Roldán, Marius Hobbhahn, and Anson. Our work involves close collaboration with other organisations, such as MIT CSAIL, Open Philanthropy, and Rethink Priorities’ AI Governance and Strategy team. 

## Our mission
Our team at Epoch seeks to clarify when and how gradually TAI capabilities will be developed.

We see these two problems as core questions for informing AI strategy decisions by grantmakers, policy-makers and technical researchers.

### Developing better forecasts about the development of TAI
Some of the biggest debates in the AI safety community stem directly from the lack of good predictions about how the development of AI will unfold, especially questions about AI timelines (Yudkowsky, 2021) and takeoff dynamics (Christiano & Cotra & Yudkowsky, 2021). The answers to these questions often have major implications – e.g. how long we have until TAI determines what kind of technical AI safety research is the most promising, and what governance strategies are the most prudent (Kokotajlo, 2021; Cotton-Barratt & Ord, 2014).

We seek to make progress on these questions, in order to inform strategic considerations by key decision makers. In general, we see these forecasts as playing an important role for downstream research. Overall, we expect that much of the value we provide will be through the datasets, techniques, models and visualisations we will create on our way to these forecasts.

### Informing AI strategy
A key bottleneck in AI governance is the high degree of uncertainty in what concrete actions should be taken (Muehlhauser, 2021). One reason for this is an insufficient understanding of the AI strategic landscape – e.g. which subfields of AI research are developing the most quickly, and which inputs of machine learning systems are the strongest drivers of performance. 

Another issue is that technological developments tend to outpace governance and policy, and the same is true with AI (Whittlestone & Clark, 2021). Governance researchers and policymakers typically lack the time to keep up with AI literature or develop deep intuitions about AI, and need ways to quickly get a high-level understanding of developments. 

We see our work as a bridge between technical developments and AI governance. We aim to answer foundational questions that have strategic implications, such as the relative importance of compute and data for building powerful AI systems. 

## Overview of our research agenda
This section provides a brief overview of our research and future plans – we plan on publishing a more detailed research agenda in the coming months.

Similar work on forecasting and monitoring AI developments has been done in the past. Among others, Open Philanthropy has been conducting “worldview investigations” (Cotra, 2020; Davidson, 2021a; Davidson, 2021b; Roodman, 2020), and AI impacts previously looked at the possibility of discontinuous progress in AI (Grace et al., 2020). We see our work as differing from earlier work in two main ways:
- We’re grounded in empirical data on AI specifically, as opposed to other fields
- We take a systems-building approach (by developing datasets, tools, insights, etc.)

We want our work to not just provide yet another answer to TAI timelines, but also set the groundwork for more research and cross-examination of models.

## Prior and current work
At present, we are investigating ways to develop better quantitative models for forecasting the development of TAI and its impacts. This has thus far involved looking at key inputs to machine learning systems, particularly parameters and compute (Sevilla et al., 2021; Sevilla et al., 2022), which are both important variables from the influential scaling laws paper by Kaplan et al. (2020). We are currently in the process of doing a similar investigation on dataset sizes, creating guidelines for acquiring relevant data, and creating a public good for related research (i.e. publicising our database). 

In the short term, we intend on tying up our investigation into direct inputs of ML models, and using our results to perform downstream investigations (e.g. understanding whether we should expect compute progress to revert back to Moore’s Law). We also intend on looking at other input factors, such as money, skill, and talent. The results of these investigations feed directly into our work on forecasting future AI developments.

## Future plans
One of our goals is to help develop new quantitative models for TAI development, and to dive into what we see as key weaknesses in existing models. We also hope to look at the outputs and capabilities of machine learning systems, e.g. by studying the role and evolution of popular AI benchmarks over time. We intend on sharing more details about these plans in the coming months. 

# Hiring
We expect to be hiring for several full-time research and management roles from May to July. Salaries range from \\$60,000 for entry roles to \\$80,000 for senior roles.

Why work with Epoch? 
- You can work from anywhere in the world, and work will primarily be done remotely We expect to organise group retreats every 6 months to do in-person coworking, and we can help with visa applications
- You get to work directly on topics that have key strategic importance
- Compared to other roles in AI safety, we do not require as much experience, and can offer a relatively easy ramp for getting into the field of AI governance/forecasting
- Going forward, we will work with prestigious organisations with which you will be affiliated, and release publications relating to our research agenda (more details in a followup hiring post)

You may be an especially good fit if you:
- Are knowledgeable about economics (particularly statistics and econometrics)
- Have experience with data science and programming (particularly scraping data)
- Are comfortable doing independent research
- Are familiar with forecasting, recent AI developments, technical AI safety, or AI governance. To get a feel for what kinds of background we think would be the most useful, see this shortform post. 

If you think you might be a good fit for us, please apply! If you’re unsure whether this is the right role for you, we strongly encourage you to apply anyway. Please register your interest for these roles at this airtable form – applications will open in May. 
